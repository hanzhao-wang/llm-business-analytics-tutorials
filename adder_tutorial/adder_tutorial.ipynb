{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 2: GPT-Adder - Learning Arithmetic with Complete Input\n",
    "\n",
    "Welcome to the GPT-Adder tutorial! In this version, we train a transformer model to perform addition where:\n",
    "- **Input (X)**: Complete question like \"2+3=\"\n",
    "- **Output (Y)**: Single predicted answer like \"5\"\n",
    "\n",
    "This is different from the original autoregressive word-by-word prediction for NLP. Instead, we treat this as a sequence-to-single-token prediction task.\n",
    "\n",
    "**Goal:** Train a transformer to map complete addition questions to single numeric answers.\n",
    "\n",
    "- Input X is the full question \"a+b=\"\n",
    "- Output Y is a single token representing the answer\n",
    "- We'll use a classification approach where each possible answer is a class\n",
    "- Model architecture includes a classification head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x119c7ab50>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from transformers import GPT2Config, GPT2Model\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# For reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 32     # How many independent sequences will we process in parallel to speed up the training process\n",
    "max_iters = 10000       # Iteration of training \n",
    "eval_interval = 250    # Interval of evaluation\n",
    "learning_rate = 5e-4   # Learning rate for the optimizer\n",
    "device = 'cpu'         # The device to run the model on\n",
    "eval_iters = 500       # The number of iterations to evaluate the model\n",
    "n_embd = 128           # The number of embedding dimensions\n",
    "n_head = 4             # The number of attention heads\n",
    "n_layer = 4            # The number of layers\n",
    "dropout = 0.1          # Dropout rate, this is to prevent overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum possible answer: 198\n",
      "Number of answer classes: 199\n"
     ]
    }
   ],
   "source": [
    "# Parameters for data generation\n",
    "ndigit = 2  # Up to 2-digit numbers (0-99)\n",
    "\n",
    "# We can view the adding problem as a classification problem, where the question is the input and the answer is the class label.\n",
    "# Calculate maximum possible answer for classification\n",
    "max_answer = (10**ndigit - 1) + (10**ndigit - 1)  # e.g., 99+99=198 for ndigit=2\n",
    "num_answer_classes = max_answer + 1  # 0 to max_answer inclusive\n",
    "\n",
    "print(f\"Maximum possible answer: {max_answer}\")\n",
    "print(f\"Number of answer classes: {num_answer_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation: Maximum Possible Answer and Classification\n",
    "\n",
    "In this tutorial, we are framing the addition problem \"a+b=\" as a **classification task**. This means the model's goal is not to generate the sequence of digits for the answer, but to predict which *single class* the answer belongs to.\n",
    "\n",
    "Think of it like image classification where a model predicts if an image is a \"cat,\" \"dog,\" or \"bird.\" Here, our \"classes\" are all the possible numerical answers the addition problems can produce.\n",
    "\n",
    "1.  **Defining the Classes:**\n",
    "    Since our input numbers `a` and `b` are limited by `ndigit` (e.g., for `ndigit=2`, numbers range from 0 to 99), there's a maximum possible sum.\n",
    "    - If `ndigit=2`, the largest sum is 99 + 99 = 198.\n",
    "    - The smallest sum is 0 + 0 = 0.\n",
    "    So, all possible answers lie in the range \\[0, 198].\n",
    "\n",
    "2.  **`num_answer_classes`:**\n",
    "    Each unique integer answer in this range becomes a distinct \"class\" for our model.\n",
    "    - `max_answer = (10**ndigit - 1) + (10**ndigit - 1)` calculates this maximum sum.\n",
    "    - `num_answer_classes = max_answer + 1` determines the total number of unique classes (from 0 up to `max_answer`, inclusive). For `ndigit=2`, this is 198 + 1 = 199 classes.\n",
    "\n",
    "3.  **Why Classification?**\n",
    "    By treating this as a classification problem:\n",
    "    - The model's output layer (the \"classification head\") will have `num_answer_classes` neurons.\n",
    "    - Each neuron corresponds to one possible sum (e.g., neuron 0 for answer \"0\", neuron 1 for answer \"1\", ..., neuron 198 for answer \"198\").\n",
    "    - The model will output a probability distribution over these classes, and the class with the highest probability is chosen as the predicted answer.\n",
    "    - We use `CrossEntropyLoss`, which is standard for classification tasks.\n",
    "\n",
    "This approach simplifies the problem compared to generating an answer character by character, especially since the output (the sum) is a single entity. The model just needs to learn to map the input question sequence to the correct answer \"bucket\" or class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation\n",
    "\n",
    "### 3.1 Vocabulary and Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input vocabulary: '0123456789+= '\n",
      "Input vocabulary size: 13\n",
      "Original question: '2+3='\n",
      "Encoded: [2, 10, 3, 11]\n",
      "Decoded: '2+3='\n"
     ]
    }
   ],
   "source": [
    "#We can view each character as a token, and the input is a sequence of tokens.\n",
    "input_chars = '0123456789+= '  # Added space at the end for padding (will be explained later)\n",
    "input_vocab_size = len(input_chars)\n",
    "print(f\"Input vocabulary: '{input_chars}'\")\n",
    "print(f\"Input vocabulary size: {input_vocab_size}\")\n",
    "\n",
    "# Create mappings for input\n",
    "input_stoi = {ch: i for i, ch in enumerate(input_chars)} #mapping from input elements to index\n",
    "input_itos = {i: ch for i, ch in enumerate(input_chars)} #mapping from index to input elements\n",
    "\n",
    "def encode_input(s):\n",
    "    return [input_stoi[c] for c in s] #encode the input string into a list of indices\n",
    "\n",
    "def decode_input(l):\n",
    "    return ''.join([input_itos[i] for i in l]) #decode the list of indices into a string\n",
    "\n",
    "# Test encoding/decoding\n",
    "test_question = \"2+3=\"\n",
    "encoded_test = encode_input(test_question)\n",
    "decoded_test = decode_input(encoded_test)\n",
    "print(f\"Original question: '{test_question}'\")\n",
    "print(f\"Encoded: {encoded_test}\")\n",
    "print(f\"Decoded: '{decoded_test}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample problems:\n",
      "Question: '81+14=' -> Answer: 95\n",
      "Question: '3+94=' -> Answer: 97\n",
      "Question: '35+31=' -> Answer: 66\n",
      "Question: '28+17=' -> Answer: 45\n",
      "Question: '94+13=' -> Answer: 107\n"
     ]
    }
   ],
   "source": [
    "def generate_addition_data(num_digits):\n",
    "    \"\"\"Generate a single addition problem and answer.\"\"\"\n",
    "    a = random.randint(0, 10**num_digits - 1)\n",
    "    b = random.randint(0, 10**num_digits - 1)\n",
    "    c = a + b\n",
    "    question = f\"{a}+{b}=\"\n",
    "    answer = c  # Single integer answer\n",
    "    return question, answer\n",
    "\n",
    "# Test data generation\n",
    "print(\"Sample problems:\")\n",
    "for _ in range(5):\n",
    "    q, a = generate_addition_data(ndigit)\n",
    "    print(f\"Question: '{q}' -> Answer: {a}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Dataset Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum question length: 6\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample question: '86+94='\n",
      "Sample answer: '180'\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample question: '69+11='\n",
      "Sample answer: '80'\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample question: '75+54='\n",
      "Sample answer: '129'\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample question: '4+3=  '\n",
      "Sample answer: '7'\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample question: '11+27='\n",
      "Sample answer: '38'\n"
     ]
    }
   ],
   "source": [
    "# Recall the training would be done in batches. However, the question length can not be the same for each batch.\n",
    "# E.g., the question \"2+3=\" is 4 characters, but the question \"99+99=\" is 6 characters.\n",
    "# So we need to pad the question to the same length for each batch.\n",
    "# This is like what we do to fill the missing values in SKlearn.\n",
    "\n",
    "# Calculate maximum question length for padding. The maximum question length is like the block_size in the nanoGPT tutorial.\n",
    "max_question_length = ndigit + 1 + ndigit + 1  # a + \"+\" + b + \"=\"\n",
    "print(f\"Maximum question length: {max_question_length}\")\n",
    "\n",
    "def get_batch():\n",
    "    \"\"\"\n",
    "    Generate a batch of padded addition problems. \n",
    "    \"\"\"\n",
    "    questions = []\n",
    "    answers = []\n",
    "    for _ in range(batch_size):\n",
    "        # Generate a new addition problem\n",
    "        q_str, ans_int = generate_addition_data(ndigit)\n",
    "        \n",
    "        # Pad and encode the question\n",
    "        # ljust(max_question_length): pad the question to the left with spaces to make it the same length as the maximum question length\n",
    "        # This is like what we do to fill the missing values in the dataset.\n",
    "        padded_q = q_str.ljust(max_question_length)\n",
    "        encoded_q = encode_input(padded_q)\n",
    "        \n",
    "        questions.append(encoded_q)\n",
    "        answers.append(ans_int)\n",
    "    \n",
    "    # Convert lists to tensors and move to the correct device\n",
    "    x = torch.tensor(questions, dtype=torch.long, device=device)\n",
    "    y = torch.tensor(answers, dtype=torch.long, device=device)\n",
    "\n",
    "    return x, y \n",
    "\n",
    "# Sample the first question and answer in the batch\n",
    "x,y = get_batch()\n",
    "for i in range(5):\n",
    "    print('-'*100)\n",
    "    print(f\"Sample question: '{decode_input(x[i].tolist())}'\")\n",
    "    print(f\"Sample answer: '{y[i]}'\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation: Padding Input Sequences\n",
    "\n",
    "Transformer models, like the GPT-2 architecture we're using as a base, are designed to process sequences of a fixed length. However, our input questions (e.g., \"1+2=\", \"10+5=\", \"99+99=\") can have varying lengths.\n",
    "\n",
    "**Why Padding?**\n",
    "1.  **Batch Processing:** To train neural networks efficiently, we feed data in batches. All sequences within a single batch must have the same length so they can be processed in parallel by the GPU or CPU.\n",
    "2.  **Fixed-Size Model Input:** The transformer architecture itself expects inputs of a predefined maximum sequence length (`n_positions` in `GPT2Config`, which we set to `max_question_length`).\n",
    "\n",
    "This is just like in Sklearn, when we want to use rows with missing value, we need to first fill these missing values to make every row has the same length.\n",
    "\n",
    "**How Padding Works:**\n",
    "1.  **Determine `max_question_length`:** We first calculate the maximum possible length an input question can have. For `ndigit=2`, the longest question is \"99+99=\" (6 characters). This becomes our `max_question_length`.\n",
    "2.  **Add a Padding Token:** We add a special padding character to our input vocabulary (in this case, a space ' ').\n",
    "3.  **Pad Shorter Sequences:** Any question shorter than `max_question_length` is padded with this special character (usually at the end) until it reaches the `max_question_length`.\n",
    "    - \"2+3=\" (length 4) with `max_question_length=6` becomes \"2+3=  \" (length 6).\n",
    "    - The `ljust(max_question_len)` method handles this.\n",
    "\n",
    "**(Optional) Attention Mechanism and Padding:**\n",
    "While the input sequences are padded, the transformer's attention mechanism can be designed (often through an attention mask) to ignore these padding tokens during computation. This ensures that the padding doesn't negatively influence the learning process. For this specific `GPT2Model` from Hugging Face, it typically handles attention masking internally based on standard padding token IDs or by allowing explicit attention masks. In our simplified setup, the model will still \"see\" the padding tokens, but their embeddings will be learned like any other token. The key is that the *structure* of the input is now uniform across the batch.\n",
    "\n",
    "This padding ensures that all input tensors passed to the model have a consistent shape, which is essential for the underlying computations and batching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Definition\n",
    "\n",
    "We'll use GPT2Model (without the language modeling head) and add our own classification head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.82 M parameters\n"
     ]
    }
   ],
   "source": [
    "#We need to define a new model to train the addition classifier.\n",
    "#Why? In the original nanoGPT tutorial, the number of classes is the same as the number of tokens in the vocabulary.\n",
    "#However, in our case, the number of classes is the number of possible answers, different from the number of tokens in the vocabulary.\n",
    "#So we can't use the original GPT2Model (with the language modeling/LM head) directly like in the nanoGPT tutorial.\n",
    "\n",
    "class AdditionClassifier(nn.Module): #This is a class that inherits from the nn.Module class in PyTorch. \n",
    "    #Basicly it tells Python that this is a neural network model and can be used to train and test with commands in PyTorch.\n",
    "    \"\"\"Transformer model for addition classification.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_vocab_size, num_classes, max_seq_len, n_embd, n_layer, n_head, dropout):\n",
    "        #__init__ is a special method in Python that is called when an object of a class is created.\n",
    "        super().__init__() #This is a way to call the __init__ method of the parent class (nn.Module) to initialize the model.\n",
    "        #the __init__ needs to define the architecture and parameters of the model.\n",
    "        \n",
    "        # GPT2 configuration for training the model from scratch\n",
    "        config = GPT2Config(\n",
    "            vocab_size=input_vocab_size,\n",
    "            n_positions=max_seq_len,\n",
    "            n_embd=n_embd,\n",
    "            n_layer=n_layer,\n",
    "            n_head=n_head,\n",
    "            resid_pdrop=dropout,\n",
    "            embd_pdrop=dropout,\n",
    "            attn_pdrop=dropout,\n",
    "            bos_token_id=None,\n",
    "            eos_token_id=None\n",
    "        )\n",
    "        \n",
    "        # Use GPT2Model (without language modeling/LM head) as a feature extractor. \n",
    "        #We can think this transformer/GPT2Model is a feature extractor, and its output is a feature vector of the input. \n",
    "        #These features would contain some information about the input, just like embeddings in the Word2vec/Net2vec.   \n",
    "        self.transformer = GPT2Model(config)\n",
    "        \n",
    "        # Classification head is a linear layer that maps the feature vector to the logits of the answer.\n",
    "        # Indeed we are just applying multinomial logistic regression to the feature vector from the transformer.\n",
    "        self.classifier = nn.Linear(n_embd, num_classes)\n",
    "        \n",
    "        #(Multi) logistic regression: Softmax(Linear(x)): Linear(x)=W*x+b\n",
    "        # x is the feature vector from the transformer.\n",
    "        \n",
    "    def forward(self, input_ids, labels=None):\n",
    "        # The forward function is to compute the loss and the logits from the input. (Get the prediction.)\n",
    "\n",
    "\n",
    "        # Get transformer outputs (the feature vector of the input)\n",
    "        transformer_outputs = self.transformer(input_ids)\n",
    "        feature_vector = transformer_outputs.last_hidden_state[:, -1, :]  # We get the last hidden state of the transformer as our feature vector.\n",
    " \n",
    "        \n",
    "        # Apply classification head/run the multinomial logistic regression (without softmax) to the feature vector from the transformer.\n",
    "        logits = self.classifier(feature_vector)  \n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fn = nn.CrossEntropyLoss() #This is the loss function for the multinomial logistic regression, and the softmax is included in the loss function.\n",
    "            loss = loss_fn(logits, labels)\n",
    "        \n",
    "        return {'loss': loss, 'logits': logits} #we define the output of the model as a dictionary, which contains the loss and the logits.\n",
    "\n",
    "# Create model\n",
    "model = AdditionClassifier(\n",
    "    input_vocab_size=input_vocab_size,\n",
    "    num_classes=num_answer_classes,\n",
    "    max_seq_len=max_question_length,\n",
    "    n_embd=n_embd,\n",
    "    n_layer=n_layer,\n",
    "    n_head=n_head,\n",
    "    dropout=dropout\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "print(f\"{sum(p.numel() for p in model.parameters())/1e6:.2f} M parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation: Custom Classification Head\n",
    "\n",
    "The standard GPT-2 model from the `transformers` library, when used as `GPT2LMHeadModel`, is designed for **language modeling**. This means its primary goal is to predict the next token in a sequence, autoregressively. It has a \"language modeling head\" which is essentially a linear layer that maps the transformer's output hidden states to logits over the entire vocabulary (to predict the next word/character).\n",
    "\n",
    "**Our Task is Different:**\n",
    "In this tutorial, we are not performing traditional language modeling. Our task is **many-to-one classification**:\n",
    "-   **Input (Many):** A sequence of characters representing an addition problem (e.g., \"23+45=\").\n",
    "-   **Output (One):** A single class label representing the numerical answer (e.g., class 68).\n",
    "\n",
    "**Why `GPT2Model` + Custom Head?**\n",
    "\n",
    "1.  **Leveraging Transformer Power:** We still want to use the powerful sequence processing capabilities of the transformer architecture (self-attention, positional encodings, etc.) to understand the input question \"23+45=\". `GPT2Model` provides the core transformer blocks (embedding layer, multiple transformer layers) without the final language modeling layer.\n",
    "\n",
    "2.  **Tailoring to Classification:**\n",
    "    -   The output of `GPT2Model` is a sequence of hidden states, one for each input token. For our classification task, we are particularly interested in the information aggregated by the transformer over the entire sequence. A common strategy is to use the hidden state of the *last* token (or a special `[CLS]` token if one were used, or an aggregation like pooling). In our case, we use the hidden state corresponding to the final input token (which is often the '=' sign or a padding token if the actual question is shorter).\n",
    "    -   This chosen hidden state (a vector of size `n_embd`) is then fed into our custom **classification head**.\n",
    "\n",
    "3.  **The `self.classifier`:**\n",
    "    Our classification head is a simple `nn.Linear` layer: `self.classifier = nn.Linear(n_embd, num_classes)`.\n",
    "    -   It takes the `n_embd`-dimensional feature vector from the transformer.\n",
    "    -   It projects this vector into a `num_classes`-dimensional space. Each dimension in this output corresponds to one of the possible numerical answers (from 0 to `max_answer`).\n",
    "    -   The output of this linear layer are the **logits** for our classification task. Applying a softmax function to these logits gives the probabilities for each possible answer class.\n",
    "\n",
    "In summary, we use `GPT2Model` as a powerful feature extractor for our input sequence and then add a simple linear layer (`self.classifier`) on top to perform the final classification into one of the `num_answer_classes`. This adapts the general-purpose transformer architecture to our specific arithmetic task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer. We use AdamW as the optimizer like nanoGPT.\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = []\n",
    "        accuracies = []\n",
    "        for _ in range(eval_iters):\n",
    "            # Generate a batch of data\n",
    "            X, Y = get_batch()\n",
    "            \n",
    "            outputs = model(X, labels=Y)\n",
    "            loss = outputs['loss']\n",
    "            logits = outputs['logits']\n",
    "            \n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            accuracy = (predictions == Y).float().mean().item()\n",
    "            accuracies.append(accuracy)\n",
    "            \n",
    "        out[split + '_loss'] = np.mean(losses)\n",
    "        out[split + '_acc'] = np.mean(accuracies)\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cpu...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 5.5264, train acc 0.0063, val loss 5.5268, val acc 0.0069\n",
      "step 250: train loss 3.9543, train acc 0.0429, val loss 3.9518, val acc 0.0426\n",
      "step 500: train loss 3.4845, train acc 0.0601, val loss 3.4849, val acc 0.0575\n",
      "step 750: train loss 3.2916, train acc 0.0643, val loss 3.2796, val acc 0.0660\n",
      "step 1000: train loss 2.9483, train acc 0.1133, val loss 2.9451, val acc 0.1142\n",
      "step 1250: train loss 2.9005, train acc 0.0975, val loss 2.8986, val acc 0.0995\n",
      "step 1500: train loss 2.6854, train acc 0.1433, val loss 2.6879, val acc 0.1359\n",
      "step 1750: train loss 2.5626, train acc 0.1527, val loss 2.5622, val acc 0.1502\n",
      "step 2000: train loss 2.5399, train acc 0.1406, val loss 2.5426, val acc 0.1416\n",
      "step 2250: train loss 2.4448, train acc 0.1704, val loss 2.4496, val acc 0.1676\n",
      "step 2500: train loss 2.3579, train acc 0.1780, val loss 2.3600, val acc 0.1791\n",
      "step 2750: train loss 2.3303, train acc 0.1704, val loss 2.3319, val acc 0.1744\n",
      "step 3000: train loss 2.3421, train acc 0.1747, val loss 2.3439, val acc 0.1706\n",
      "step 3250: train loss 2.3303, train acc 0.1639, val loss 2.3337, val acc 0.1669\n",
      "step 3500: train loss 2.2421, train acc 0.1762, val loss 2.2364, val acc 0.1821\n",
      "step 3750: train loss 2.1482, train acc 0.2088, val loss 2.1417, val acc 0.2154\n",
      "step 4000: train loss 2.1599, train acc 0.2058, val loss 2.1662, val acc 0.2040\n",
      "step 4250: train loss 2.2515, train acc 0.1657, val loss 2.2430, val acc 0.1663\n",
      "step 4500: train loss 2.0701, train acc 0.2412, val loss 2.0734, val acc 0.2339\n",
      "step 4750: train loss 2.0037, train acc 0.2532, val loss 2.0105, val acc 0.2494\n",
      "step 5000: train loss 2.0095, train acc 0.2407, val loss 2.0192, val acc 0.2378\n",
      "step 5250: train loss 2.0199, train acc 0.2367, val loss 2.0177, val acc 0.2354\n",
      "step 5500: train loss 1.9180, train acc 0.2759, val loss 1.9169, val acc 0.2715\n",
      "step 5750: train loss 1.8979, train acc 0.2744, val loss 1.8911, val acc 0.2782\n",
      "step 6000: train loss 1.8301, train acc 0.2782, val loss 1.8249, val acc 0.2863\n",
      "step 6250: train loss 1.7586, train acc 0.3328, val loss 1.7517, val acc 0.3401\n",
      "step 6500: train loss 1.5352, train acc 0.4101, val loss 1.5361, val acc 0.4093\n",
      "step 6750: train loss 1.3687, train acc 0.4733, val loss 1.3714, val acc 0.4715\n",
      "step 7000: train loss 1.2480, train acc 0.5078, val loss 1.2422, val acc 0.5153\n",
      "step 7250: train loss 1.1624, train acc 0.5299, val loss 1.1691, val acc 0.5258\n",
      "step 7500: train loss 0.9933, train acc 0.6462, val loss 0.9867, val acc 0.6494\n",
      "step 7750: train loss 0.9005, train acc 0.6934, val loss 0.8989, val acc 0.6926\n",
      "step 8000: train loss 0.7819, train acc 0.7465, val loss 0.7797, val acc 0.7491\n",
      "step 8250: train loss 0.7338, train acc 0.7621, val loss 0.7328, val acc 0.7633\n",
      "step 8500: train loss 0.7454, train acc 0.7452, val loss 0.7440, val acc 0.7502\n",
      "step 8750: train loss 0.5953, train acc 0.8357, val loss 0.5949, val acc 0.8344\n",
      "step 9000: train loss 0.4732, train acc 0.9146, val loss 0.4686, val acc 0.9144\n",
      "step 9250: train loss 0.4450, train acc 0.8788, val loss 0.4501, val acc 0.8762\n",
      "step 9500: train loss 0.3208, train acc 0.9414, val loss 0.3296, val acc 0.9335\n",
      "step 9750: train loss 0.3168, train acc 0.9283, val loss 0.3143, val acc 0.9283\n",
      "step 9999: train loss 0.2362, train acc 0.9503, val loss 0.2380, val acc 0.9494\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "print(f\"Training on {device}...\")\n",
    "\n",
    "for iter_num in range(max_iters):\n",
    "    # Evaluation part\n",
    "    if iter_num % eval_interval == 0 or iter_num == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter_num}: train loss {losses.get('train_loss', float('nan')):.4f}, train acc {losses.get('train_acc', float('nan')):.4f}, val loss {losses.get('val_loss', float('nan')):.4f}, val acc {losses.get('val_acc', float('nan')):.4f}\")\n",
    "\n",
    "    # Training part\n",
    "\n",
    "    # 1. Get a batch of data\n",
    "    questions, answers = get_batch()\n",
    "\n",
    "    # 2. Forward pass\n",
    "    outputs = model(questions, labels=answers)\n",
    "    \n",
    "    # 3. Calculate loss\n",
    "    loss = outputs['loss']\n",
    "\n",
    "    # 4 and 5. Backward pass and update parameters\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Testing and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Testing model on 30 examples (up to 2-digit numbers) ---\n",
      "Problem  1: 33+35=68 -> Model predicted: 69 -> INCORRECT\n",
      "Problem  2: 50+88=138 -> Model predicted: 138 -> CORRECT\n",
      "Problem  3: 36+28=64 -> Model predicted: 64 -> CORRECT\n",
      "Problem  4: 10+63=73 -> Model predicted: 73 -> CORRECT\n",
      "Problem  5: 33+3=36 -> Model predicted: 36 -> CORRECT\n",
      "Problem  6: 9+25=34 -> Model predicted: 34 -> CORRECT\n",
      "Problem  7: 84+16=100 -> Model predicted: 100 -> CORRECT\n",
      "Problem  8: 40+88=128 -> Model predicted: 128 -> CORRECT\n",
      "Problem  9: 63+96=159 -> Model predicted: 159 -> CORRECT\n",
      "Problem 10: 0+60=60 -> Model predicted: 61 -> INCORRECT\n",
      "Problem 11: 4+55=59 -> Model predicted: 59 -> CORRECT\n",
      "Problem 12: 14+43=57 -> Model predicted: 57 -> CORRECT\n",
      "Problem 13: 92+12=104 -> Model predicted: 104 -> CORRECT\n",
      "Problem 14: 57+13=70 -> Model predicted: 70 -> CORRECT\n",
      "Problem 15: 86+97=183 -> Model predicted: 183 -> CORRECT\n",
      "Problem 16: 98+42=140 -> Model predicted: 140 -> CORRECT\n",
      "Problem 17: 93+30=123 -> Model predicted: 123 -> CORRECT\n",
      "Problem 18: 6+50=56 -> Model predicted: 56 -> CORRECT\n",
      "Problem 19: 59+70=129 -> Model predicted: 129 -> CORRECT\n",
      "Problem 20: 96+96=192 -> Model predicted: 193 -> INCORRECT\n",
      "Problem 21: 26+40=66 -> Model predicted: 66 -> CORRECT\n",
      "Problem 22: 21+63=84 -> Model predicted: 84 -> CORRECT\n",
      "Problem 23: 14+13=27 -> Model predicted: 28 -> INCORRECT\n",
      "Problem 24: 14+49=63 -> Model predicted: 63 -> CORRECT\n",
      "Problem 25: 48+15=63 -> Model predicted: 63 -> CORRECT\n",
      "Problem 26: 85+27=112 -> Model predicted: 112 -> CORRECT\n",
      "Problem 27: 96+45=141 -> Model predicted: 141 -> CORRECT\n",
      "Problem 28: 89+43=132 -> Model predicted: 132 -> CORRECT\n",
      "Problem 29: 62+47=109 -> Model predicted: 109 -> CORRECT\n",
      "Problem 30: 94+23=117 -> Model predicted: 117 -> CORRECT\n",
      "Accuracy: 86.67% (26/30 correct)\n"
     ]
    }
   ],
   "source": [
    "def test_model_addition(num_tests=20, num_digits_test=ndigit):\n",
    "    print(f\"--- Testing model on {num_tests} examples (up to {num_digits_test}-digit numbers) ---\")\n",
    "    model.eval()\n",
    "    correct_predictions = 0\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_tests):\n",
    "            # Generate test problem\n",
    "            a = random.randint(0, 10**num_digits_test - 1)\n",
    "            b = random.randint(0, 10**num_digits_test - 1)\n",
    "            correct_answer = a + b\n",
    "            question = f\"{a}+{b}=\"\n",
    "            \n",
    "            # Pad and encode question\n",
    "            padded_question = question.ljust(max_question_length)\n",
    "            encoded_question = torch.tensor(encode_input(padded_question), dtype=torch.long, device=device).unsqueeze(0) #unsqueeze(0): add a dimension at the beginning of the tensor to match the format of the training data\n",
    "            \n",
    "            # Get model prediction\n",
    "            \n",
    "            outputs = model(encoded_question)\n",
    "            logits = outputs['logits']\n",
    "            predicted_answer = torch.argmax(logits, dim=-1).item()\n",
    "            \n",
    "            is_correct = (predicted_answer == correct_answer)\n",
    "            if is_correct:\n",
    "                correct_predictions += 1\n",
    "                status = \"CORRECT\"\n",
    "            else:\n",
    "                status = \"INCORRECT\"\n",
    "            \n",
    "            print(f\"Problem {i+1:2d}: {question}{correct_answer} -> Model predicted: {predicted_answer} -> {status}\")\n",
    "    \n",
    "    accuracy = (correct_predictions / num_tests) * 100\n",
    "    print(f\"Accuracy: {accuracy:.2f}% ({correct_predictions}/{num_tests} correct)\")\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "# Run test\n",
    "test_model_addition(num_tests=30, num_digits_test=ndigit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Interactive Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Interactive Testing ===\n",
      "Question: 2+3=\n",
      "Predicted answer: 5\n",
      "Top 3 predictions:\n",
      "  5: 0.229\n",
      "  8: 0.136\n",
      "  6: 0.096\n",
      "\n",
      "Question: 5+4=\n",
      "Predicted answer: 10\n",
      "Top 3 predictions:\n",
      "  10: 0.196\n",
      "  11: 0.191\n",
      "  8: 0.160\n",
      "\n",
      "Question: 9+9=\n",
      "Predicted answer: 15\n",
      "Top 3 predictions:\n",
      "  15: 0.236\n",
      "  13: 0.166\n",
      "  12: 0.114\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ask_adder(problem_input):\n",
    "    \"\"\"Ask the model to solve an addition problem.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Ensure input ends with '='\n",
    "    if not problem_input.endswith('='):\n",
    "        question = problem_input + '='\n",
    "    else:\n",
    "        question = problem_input\n",
    "    \n",
    "    # Pad and encode\n",
    "    padded_question = question.ljust(max_question_length)\n",
    "    encoded_question = torch.tensor(encode_input(padded_question), dtype=torch.long, device=device).unsqueeze(0)\n",
    "    \n",
    "    # Get prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model(encoded_question)\n",
    "        logits = outputs['logits']\n",
    "        predicted_answer = torch.argmax(logits, dim=-1).item()\n",
    "        \n",
    "        # Also get top-3 predictions with probabilities\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        top_probs, top_indices = torch.topk(probs, k=3, dim=-1)\n",
    "        \n",
    "    model.train()\n",
    "    \n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Predicted answer: {predicted_answer}\")\n",
    "    print(\"Top 3 predictions:\")\n",
    "    for i in range(3):\n",
    "        ans = top_indices[0][i].item()\n",
    "        prob = top_probs[0][i].item()\n",
    "        print(f\"  {ans}: {prob:.3f}\")\n",
    "    \n",
    "    return predicted_answer\n",
    "\n",
    "# Test examples\n",
    "print(\"=== Interactive Testing ===\")\n",
    "ask_adder('2+3')\n",
    "print()\n",
    "ask_adder('5+4')\n",
    "print()\n",
    "ask_adder('9+9')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GenAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
