{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 2: GPT-Adder - Learning Arithmetic with Complete Input\n",
    "\n",
    "Welcome to the GPT-Adder tutorial! In this version, we train a transformer model to perform addition where:\n",
    "- **Input (X)**: Complete question like \"2+3=\"\n",
    "- **Output (Y)**: Single predicted answer like \"5\"\n",
    "\n",
    "This is different from the original autoregressive character-by-character prediction for NLP. Instead, we treat this as a sequence-to-single-token prediction task.\n",
    "\n",
    "**Goal:** Train a transformer to map complete addition questions to single numeric answers.\n",
    "\n",
    "- Input X is the full question \"a+b=\"\n",
    "- Output Y is a single token representing the answer\n",
    "- We'll use a classification approach where each possible answer is a class\n",
    "- Model architecture includes a classification head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Config, GPT2Model\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum possible answer: 198\n",
      "Number of answer classes: 199\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1155373b0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 32     # How many independent sequences will we process in parallel to speed up the training process\n",
    "max_iters = 2000       # Iteration of training \n",
    "eval_interval = 250    # Interval of evaluation\n",
    "learning_rate = 1e-3   # Learning rate for the optimizer\n",
    "device = 'cpu'         # The device to run the model on\n",
    "eval_iters = 100       # The number of iterations to evaluate the model\n",
    "n_embd = 128           # The number of embedding dimensions\n",
    "n_head = 4             # The number of attention heads\n",
    "n_layer = 4            # The number of layers\n",
    "dropout = 0.1          # Dropout rate, this is to prevent overfitting\n",
    "\n",
    "# Parameters for data generation\n",
    "ndigit = 2  # Up to 2-digit numbers (0-99)\n",
    "\n",
    "# Calculate maximum possible answer for classification\n",
    "max_answer = (10**ndigit - 1) + (10**ndigit - 1)  # e.g., 99+99=198 for ndigit=2\n",
    "num_answer_classes = max_answer + 1  # 0 to max_answer inclusive\n",
    "\n",
    "print(f\"Maximum possible answer: {max_answer}\")\n",
    "print(f\"Number of answer classes: {num_answer_classes}\")\n",
    "\n",
    "# For reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation: Maximum Possible Answer and Classification\n",
    "\n",
    "In this tutorial, we are framing the addition problem \"a+b=\" as a **classification task**. This means the model's goal is not to generate the sequence of digits for the answer, but to predict which *single class* the answer belongs to.\n",
    "\n",
    "Think of it like image classification where a model predicts if an image is a \"cat,\" \"dog,\" or \"bird.\" Here, our \"classes\" are all the possible numerical answers the addition problems can produce.\n",
    "\n",
    "1.  **Defining the Classes:**\n",
    "    Since our input numbers `a` and `b` are limited by `ndigit` (e.g., for `ndigit=2`, numbers range from 0 to 99), there's a maximum possible sum.\n",
    "    - If `ndigit=2`, the largest sum is 99 + 99 = 198.\n",
    "    - The smallest sum is 0 + 0 = 0.\n",
    "    So, all possible answers lie in the range \\[0, 198].\n",
    "\n",
    "2.  **`num_answer_classes`:**\n",
    "    Each unique integer answer in this range becomes a distinct \"class\" for our model.\n",
    "    - `max_answer = (10**ndigit - 1) + (10**ndigit - 1)` calculates this maximum sum.\n",
    "    - `num_answer_classes = max_answer + 1` determines the total number of unique classes (from 0 up to `max_answer`, inclusive). For `ndigit=2`, this is 198 + 1 = 199 classes.\n",
    "\n",
    "3.  **Why Classification?**\n",
    "    By treating this as a classification problem:\n",
    "    - The model's output layer (the \"classification head\") will have `num_answer_classes` neurons.\n",
    "    - Each neuron corresponds to one possible sum (e.g., neuron 0 for answer \"0\", neuron 1 for answer \"1\", ..., neuron 198 for answer \"198\").\n",
    "    - The model will output a probability distribution over these classes, and the class with the highest probability is chosen as the predicted answer.\n",
    "    - We use `CrossEntropyLoss`, which is standard for classification tasks.\n",
    "\n",
    "This approach simplifies the problem compared to generating an answer character by character, especially since the output (the sum) is a single entity. The model just needs to learn to map the input question sequence to the correct answer \"bucket\" or class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation\n",
    "\n",
    "### 3.1 Vocabulary and Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input vocabulary: '0123456789+= '\n",
      "Input vocabulary size: 13\n",
      "Original question: '2+3='\n",
      "Encoded: [2, 10, 3, 11]\n",
      "Decoded: '2+3='\n",
      "Padded question: '2+3= '\n",
      "Encoded padded: [2, 10, 3, 11, 12]\n",
      "Decoded padded: '2+3= '\n"
     ]
    }
   ],
   "source": [
    "input_chars = '0123456789+= '  # Added space at the end for padding\n",
    "input_vocab_size = len(input_chars)\n",
    "print(f\"Input vocabulary: '{input_chars}'\")\n",
    "print(f\"Input vocabulary size: {input_vocab_size}\")\n",
    "\n",
    "# Create mappings for input\n",
    "input_stoi = {ch: i for i, ch in enumerate(input_chars)} #mapping from input elements to index\n",
    "input_itos = {i: ch for i, ch in enumerate(input_chars)} #mapping from index to input elements\n",
    "\n",
    "def encode_input(s):\n",
    "    return [input_stoi[c] for c in s] #encode the input string into a list of indices\n",
    "\n",
    "def decode_input(l):\n",
    "    return ''.join([input_itos[i] for i in l]) #decode the list of indices into a string\n",
    "\n",
    "# Test encoding/decoding\n",
    "test_question = \"2+3=\"\n",
    "encoded_test = encode_input(test_question)\n",
    "decoded_test = decode_input(encoded_test)\n",
    "print(f\"Original question: '{test_question}'\")\n",
    "print(f\"Encoded: {encoded_test}\")\n",
    "print(f\"Decoded: '{decoded_test}'\")\n",
    "\n",
    "# Test with padding\n",
    "test_padded = \"2+3= \"  # With space padding\n",
    "encoded_padded = encode_input(test_padded)\n",
    "decoded_padded = decode_input(encoded_padded)\n",
    "print(f\"Padded question: '{test_padded}'\")\n",
    "print(f\"Encoded padded: {encoded_padded}\")\n",
    "print(f\"Decoded padded: '{decoded_padded}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum question length: 6\n",
      "Sample problems:\n",
      "Question: '81+14=' -> Answer: 95\n",
      "Question: '3+94=' -> Answer: 97\n",
      "Question: '35+31=' -> Answer: 66\n",
      "Question: '28+17=' -> Answer: 45\n",
      "Question: '94+13=' -> Answer: 107\n"
     ]
    }
   ],
   "source": [
    "def generate_addition_data(num_digits):\n",
    "    \"\"\"Generate a single addition problem and answer.\"\"\"\n",
    "    a = random.randint(0, 10**num_digits - 1)\n",
    "    b = random.randint(0, 10**num_digits - 1)\n",
    "    c = a + b\n",
    "    question = f\"{a}+{b}=\"\n",
    "    answer = c  # Single integer answer\n",
    "    return question, answer\n",
    "\n",
    "# Calculate maximum question length for padding\n",
    "# For ndigit=1: max question is \"9+9=\" (4 characters)\n",
    "max_question_length = ndigit + 1 + ndigit + 1  # a + \"+\" + b + \"=\"\n",
    "print(f\"Maximum question length: {max_question_length}\")\n",
    "\n",
    "# Test data generation\n",
    "print(\"Sample problems:\")\n",
    "for _ in range(5):\n",
    "    q, a = generate_addition_data(ndigit)\n",
    "    print(f\"Question: '{q}' -> Answer: {a}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation: Padding Input Sequences\n",
    "\n",
    "Transformer models, like the GPT-2 architecture we're using as a base, are designed to process sequences of a fixed length. However, our input questions (e.g., \"1+2=\", \"10+5=\", \"99+99=\") can have varying lengths.\n",
    "\n",
    "**Why Padding?**\n",
    "1.  **Batch Processing:** To train neural networks efficiently, we feed data in batches. All sequences within a single batch must have the same length so they can be processed in parallel by the GPU or CPU.\n",
    "2.  **Fixed-Size Model Input:** The transformer architecture itself expects inputs of a predefined maximum sequence length (`n_positions` in `GPT2Config`, which we set to `max_question_length`).\n",
    "\n",
    "**How Padding Works:**\n",
    "1.  **Determine `max_question_length`:** We first calculate the maximum possible length an input question can have. For `ndigit=2`, the longest question is \"99+99=\" (6 characters). This becomes our `max_question_length`.\n",
    "2.  **Add a Padding Token:** We add a special padding character to our input vocabulary (in this case, a space ' ').\n",
    "3.  **Pad Shorter Sequences:** Any question shorter than `max_question_length` is padded with this special character (usually at the end) until it reaches the `max_question_length`.\n",
    "    - \"2+3=\" (length 4) with `max_question_length=6` becomes \"2+3=  \" (length 6).\n",
    "    - The `ljust(max_question_len)` method in the `ModifiedAdditionDataset` handles this.\n",
    "\n",
    "**Attention Mechanism and Padding:**\n",
    "While the input sequences are padded, the transformer's attention mechanism can be designed (often through an attention mask) to ignore these padding tokens during computation. This ensures that the padding doesn't negatively influence the learning process. For this specific `GPT2Model` from Hugging Face, it typically handles attention masking internally based on standard padding token IDs or by allowing explicit attention masks. In our simplified setup, the model will still \"see\" the padding tokens, but their embeddings will be learned like any other token. The key is that the *structure* of the input is now uniform across the batch.\n",
    "\n",
    "This padding ensures that all input tensors passed to the model have a consistent shape, which is essential for the underlying computations and batching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample question tensor: tensor([ 8,  6, 10,  9,  4, 11])\n",
      "Sample question decoded: '86+94='\n",
      "Sample answer: 180\n",
      "Question tensor shape: torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "class ModifiedAdditionDataset(Dataset):\n",
    "    \"\"\"Dataset where X is complete question and Y is single answer.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_digits, num_samples, max_question_len):\n",
    "        self.num_digits = num_digits\n",
    "        self.num_samples = num_samples\n",
    "        self.max_question_len = max_question_len\n",
    "        \n",
    "        self.questions = []\n",
    "        self.answers = []\n",
    "        \n",
    "        for _ in range(num_samples):\n",
    "            question, answer = generate_addition_data(num_digits)\n",
    "            \n",
    "            # Pad question to consistent length. This is to make sure each question has the same length in the data batch. Thus, the model can handle the data batch in parallel.\n",
    "            padded_question = question.ljust(max_question_len)\n",
    "            encoded_question = encode_input(padded_question)\n",
    "            \n",
    "            self.questions.append(torch.tensor(encoded_question, dtype=torch.long)) #convert the encoded question into a tensor\n",
    "            self.answers.append(answer)  # Keep as integer for classification\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.questions[idx], self.answers[idx]\n",
    "\n",
    "# Create datasets\n",
    "train_dataset_size = 5000  # The size of the training dataset\n",
    "val_dataset_size = 500    # The size of the validation dataset\n",
    "\n",
    "\n",
    "train_dataset = ModifiedAdditionDataset(ndigit, train_dataset_size, max_question_length)\n",
    "val_dataset = ModifiedAdditionDataset(ndigit, val_dataset_size, max_question_length)\n",
    "\n",
    "# Test the dataset\n",
    "sample_question, sample_answer = train_dataset[0]\n",
    "print(f\"Sample question tensor: {sample_question}\")\n",
    "print(f\"Sample question decoded: '{decode_input(sample_question.tolist())}'\")\n",
    "print(f\"Sample answer: {sample_answer}\")\n",
    "print(f\"Question tensor shape: {sample_question.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Questions batch shape: torch.Size([32, 6])\n",
      "Answers batch shape: torch.Size([32])\n",
      "First question in batch: '32+27='\n",
      "First answer in batch: 59\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "# Test the dataloader\n",
    "questions_batch, answers_batch = next(iter(train_dataloader))\n",
    "print(f\"Questions batch shape: {questions_batch.shape}\")\n",
    "print(f\"Answers batch shape: {answers_batch.shape}\")\n",
    "print(f\"First question in batch: '{decode_input(questions_batch[0].tolist())}'\")\n",
    "print(f\"First answer in batch: {answers_batch[0].item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Definition\n",
    "\n",
    "We'll use GPT2Model (without the language modeling head) and add our own classification head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.82 M parameters\n"
     ]
    }
   ],
   "source": [
    "class AdditionClassifier(nn.Module):\n",
    "    \"\"\"Transformer model for addition classification.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_vocab_size, num_classes, max_seq_len, n_embd, n_layer, n_head, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        # GPT2 configuration for training the model from scratch\n",
    "        config = GPT2Config(\n",
    "            vocab_size=input_vocab_size,\n",
    "            n_positions=max_seq_len,\n",
    "            n_embd=n_embd,\n",
    "            n_layer=n_layer,\n",
    "            n_head=n_head,\n",
    "            resid_pdrop=dropout,\n",
    "            embd_pdrop=dropout,\n",
    "            attn_pdrop=dropout,\n",
    "            bos_token_id=None,\n",
    "            eos_token_id=None\n",
    "        )\n",
    "        \n",
    "        # Use GPT2Model (without LM head)\n",
    "        self.transformer = GPT2Model(config)\n",
    "        \n",
    "        # Classification head is a linear layer that maps the last hidden state to the number of classes (i.e. the number of possible answers from 0 to max_answer)\n",
    "        self.classifier = nn.Linear(n_embd, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input_ids, labels=None):\n",
    "        # Get transformer outputs\n",
    "        transformer_outputs = self.transformer(input_ids)\n",
    "        hidden_states = transformer_outputs.last_hidden_state  # [batch_size, seq_len, n_embd]\n",
    "        \n",
    "        # Use the last token's representation for classification\n",
    "        # This corresponds to the \"=\" token in \"a+b=\"\n",
    "        last_hidden = hidden_states[:, -1, :]  # [batch_size, n_embd]\n",
    "        \n",
    "        # Apply classification head\n",
    "        logits = self.classifier(self.dropout(last_hidden))  # [batch_size, num_classes]\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            loss = loss_fn(logits, labels)\n",
    "        \n",
    "        return {'loss': loss, 'logits': logits}\n",
    "\n",
    "# Create model\n",
    "model = AdditionClassifier(\n",
    "    input_vocab_size=input_vocab_size,\n",
    "    num_classes=num_answer_classes,\n",
    "    max_seq_len=max_question_length,\n",
    "    n_embd=n_embd,\n",
    "    n_layer=n_layer,\n",
    "    n_head=n_head,\n",
    "    dropout=dropout\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "print(f\"{sum(p.numel() for p in model.parameters())/1e6:.2f} M parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation: Custom Classification Head\n",
    "\n",
    "The standard GPT-2 model from the `transformers` library, when used as `GPT2LMHeadModel`, is designed for **language modeling**. This means its primary goal is to predict the next token in a sequence, autoregressively. It has a \"language modeling head\" which is essentially a linear layer that maps the transformer's output hidden states to logits over the entire vocabulary (to predict the next word/character).\n",
    "\n",
    "**Our Task is Different:**\n",
    "In this tutorial, we are not performing traditional language modeling. Our task is **many-to-one classification**:\n",
    "-   **Input (Many):** A sequence of characters representing an addition problem (e.g., \"23+45=\").\n",
    "-   **Output (One):** A single class label representing the numerical answer (e.g., class 68).\n",
    "\n",
    "**Why `GPT2Model` + Custom Head?**\n",
    "\n",
    "1.  **Leveraging Transformer Power:** We still want to use the powerful sequence processing capabilities of the transformer architecture (self-attention, positional encodings, etc.) to understand the input question \"23+45=\". `GPT2Model` provides the core transformer blocks (embedding layer, multiple transformer layers) without the final language modeling layer.\n",
    "\n",
    "2.  **Tailoring to Classification:**\n",
    "    -   The output of `GPT2Model` is a sequence of hidden states, one for each input token. For our classification task, we are particularly interested in the information aggregated by the transformer over the entire sequence. A common strategy is to use the hidden state of the *last* token (or a special `[CLS]` token if one were used, or an aggregation like pooling). In our case, we use the hidden state corresponding to the final input token (which is often the '=' sign or a padding token if the actual question is shorter).\n",
    "    -   This chosen hidden state (a vector of size `n_embd`) is then fed into our custom **classification head**.\n",
    "\n",
    "3.  **The `self.classifier`:**\n",
    "    Our classification head is a simple `nn.Linear` layer: `self.classifier = nn.Linear(n_embd, num_classes)`.\n",
    "    -   It takes the `n_embd`-dimensional feature vector from the transformer.\n",
    "    -   It projects this vector into a `num_classes`-dimensional space. Each dimension in this output corresponds to one of the possible numerical answers (from 0 to `max_answer`).\n",
    "    -   The output of this linear layer are the **logits** for our classification task. Applying a softmax function to these logits gives the probabilities for each possible answer class.\n",
    "\n",
    "In summary, we use `GPT2Model` as a powerful feature extractor for our input sequence and then add a simple linear layer (`self.classifier`) on top to perform the final classification into one of the `num_answer_classes`. This adapts the general-purpose transformer architecture to our specific arithmetic task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer. AdamW is a variant of the Adam optimizer that uses weight decay and is widely used in the transformer community.\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "@torch.no_grad() # This decorator is used to disable gradient calculation during the evaluation process.\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    \n",
    "    for split_name, dataloader in [('train', train_dataloader), ('val', val_dataloader)]:\n",
    "        losses = []\n",
    "        accuracies = []\n",
    "        \n",
    "        data_iter = iter(dataloader)\n",
    "        for k in range(min(eval_iters, len(dataloader))):\n",
    "            try:\n",
    "                questions, answers = next(data_iter)\n",
    "                questions, answers = questions.to(device), answers.to(device)\n",
    "                \n",
    "                outputs = model(questions, labels=answers)\n",
    "                loss = outputs['loss']\n",
    "                logits = outputs['logits']\n",
    "                \n",
    "                losses.append(loss.item())\n",
    "                \n",
    "                # Calculate accuracy\n",
    "                predictions = torch.argmax(logits, dim=-1)\n",
    "                accuracy = (predictions == answers).float().mean().item()\n",
    "                accuracies.append(accuracy)\n",
    "                \n",
    "            except StopIteration:\n",
    "                break\n",
    "        \n",
    "        if losses:\n",
    "            out[split_name + '_loss'] = np.mean(losses)\n",
    "            out[split_name + '_acc'] = np.mean(accuracies)\n",
    "        else:\n",
    "            out[split_name + '_loss'] = float('nan')\n",
    "            out[split_name + '_acc'] = float('nan')\n",
    "    \n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cpu...\n",
      "step 0: train loss 5.4547, train acc 0.0047, val loss 5.4531, val acc 0.0063\n",
      "step 250: train loss 3.8255, train acc 0.0591, val loss 3.8705, val acc 0.0479\n",
      "step 500: train loss 3.4196, train acc 0.0878, val loss 3.4543, val acc 0.0708\n",
      "step 750: train loss 3.1685, train acc 0.0884, val loss 3.2182, val acc 0.0667\n",
      "step 1000: train loss 2.9683, train acc 0.1131, val loss 3.0286, val acc 0.0938\n",
      "step 1250: train loss 2.8491, train acc 0.1194, val loss 2.9258, val acc 0.1125\n",
      "step 1500: train loss 2.8503, train acc 0.1178, val loss 2.8858, val acc 0.0771\n",
      "step 1750: train loss 2.7698, train acc 0.1244, val loss 2.8880, val acc 0.0917\n",
      "step 1999: train loss 2.5739, train acc 0.1750, val loss 2.7087, val acc 0.1417\n",
      "Training finished!\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "print(f\"Training on {device}...\")\n",
    "train_iter = iter(train_dataloader)\n",
    "\n",
    "for iter_num in range(max_iters):\n",
    "    # Evaluate\n",
    "    if iter_num % eval_interval == 0 or iter_num == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter_num}: train loss {losses.get('train_loss', float('nan')):.4f}, train acc {losses.get('train_acc', float('nan')):.4f}, val loss {losses.get('val_loss', float('nan')):.4f}, val acc {losses.get('val_acc', float('nan')):.4f}\")\n",
    "\n",
    "    # Get batch\n",
    "    try:\n",
    "        questions, answers = next(train_iter)\n",
    "    except StopIteration:\n",
    "        train_iter = iter(train_dataloader)\n",
    "        questions, answers = next(train_iter)\n",
    "    \n",
    "    questions, answers = questions.to(device), answers.to(device)\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(questions, labels=answers)\n",
    "    loss = outputs['loss']\n",
    "\n",
    "    # Backward pass\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(\"Training finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Testing and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Testing model on 20 examples (up to 2-digit numbers) ---\n",
      "Problem  1: 76+29=105 -> Model predicted: 103 -> INCORRECT\n",
      "Problem  2: 44+38=82 -> Model predicted: 76 -> INCORRECT\n",
      "Problem  3: 36+85=121 -> Model predicted: 120 -> INCORRECT\n",
      "Problem  4: 91+73=164 -> Model predicted: 165 -> INCORRECT\n",
      "Problem  5: 86+17=103 -> Model predicted: 103 -> CORRECT\n",
      "Problem  6: 91+48=139 -> Model predicted: 135 -> INCORRECT\n",
      "Problem  7: 66+94=160 -> Model predicted: 161 -> INCORRECT\n",
      "Problem  8: 31+13=44 -> Model predicted: 42 -> INCORRECT\n",
      "Problem  9: 41+66=107 -> Model predicted: 105 -> INCORRECT\n",
      "Problem 10: 74+24=98 -> Model predicted: 97 -> INCORRECT\n",
      "Problem 11: 99+46=145 -> Model predicted: 144 -> INCORRECT\n",
      "Problem 12: 58+12=70 -> Model predicted: 67 -> INCORRECT\n",
      "Problem 13: 18+91=109 -> Model predicted: 109 -> CORRECT\n",
      "Problem 14: 54+52=106 -> Model predicted: 105 -> INCORRECT\n",
      "Problem 15: 10+11=21 -> Model predicted: 26 -> INCORRECT\n",
      "Problem 16: 7+54=61 -> Model predicted: 67 -> INCORRECT\n",
      "Problem 17: 16+60=76 -> Model predicted: 76 -> CORRECT\n",
      "Problem 18: 27+57=84 -> Model predicted: 81 -> INCORRECT\n",
      "Problem 19: 9+47=56 -> Model predicted: 55 -> INCORRECT\n",
      "Problem 20: 86+67=153 -> Model predicted: 149 -> INCORRECT\n",
      "Accuracy: 15.00% (3/20 correct)\n"
     ]
    }
   ],
   "source": [
    "def test_model_addition(num_tests=20, num_digits_test=ndigit):\n",
    "    print(f\"--- Testing model on {num_tests} examples (up to {num_digits_test}-digit numbers) ---\")\n",
    "    model.eval()\n",
    "    correct_predictions = 0\n",
    "\n",
    "    for i in range(num_tests):\n",
    "        # Generate test problem\n",
    "        a = random.randint(0, 10**num_digits_test - 1)\n",
    "        b = random.randint(0, 10**num_digits_test - 1)\n",
    "        correct_answer = a + b\n",
    "        question = f\"{a}+{b}=\"\n",
    "        \n",
    "        # Pad and encode question\n",
    "        padded_question = question.ljust(max_question_length)\n",
    "        encoded_question = torch.tensor(encode_input(padded_question), dtype=torch.long, device=device).unsqueeze(0)\n",
    "        \n",
    "        # Get model prediction\n",
    "        with torch.no_grad():\n",
    "            outputs = model(encoded_question)\n",
    "            logits = outputs['logits']\n",
    "            predicted_answer = torch.argmax(logits, dim=-1).item()\n",
    "        \n",
    "        is_correct = (predicted_answer == correct_answer)\n",
    "        if is_correct:\n",
    "            correct_predictions += 1\n",
    "            status = \"CORRECT\"\n",
    "        else:\n",
    "            status = \"INCORRECT\"\n",
    "        \n",
    "        print(f\"Problem {i+1:2d}: {question}{correct_answer} -> Model predicted: {predicted_answer} -> {status}\")\n",
    "    \n",
    "    accuracy = (correct_predictions / num_tests) * 100\n",
    "    print(f\"Accuracy: {accuracy:.2f}% ({correct_predictions}/{num_tests} correct)\")\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "# Run test\n",
    "test_model_addition(num_tests=20, num_digits_test=ndigit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Interactive Testing ===\n",
      "Question: 2+3=\n",
      "Predicted answer: 6\n",
      "Top 3 predictions:\n",
      "  6: 0.092\n",
      "  14: 0.091\n",
      "  18: 0.086\n",
      "\n",
      "Question: 5+4=\n",
      "Predicted answer: 9\n",
      "Top 3 predictions:\n",
      "  9: 0.130\n",
      "  14: 0.103\n",
      "  6: 0.078\n",
      "\n",
      "Question: 9+9=\n",
      "Predicted answer: 14\n",
      "Top 3 predictions:\n",
      "  14: 0.110\n",
      "  9: 0.096\n",
      "  11: 0.085\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ask_adder(problem_input):\n",
    "    \"\"\"Ask the model to solve an addition problem.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Ensure input ends with '='\n",
    "    if not problem_input.endswith('='):\n",
    "        question = problem_input + '='\n",
    "    else:\n",
    "        question = problem_input\n",
    "    \n",
    "    # Pad and encode\n",
    "    padded_question = question.ljust(max_question_length)\n",
    "    encoded_question = torch.tensor(encode_input(padded_question), dtype=torch.long, device=device).unsqueeze(0)\n",
    "    \n",
    "    # Get prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model(encoded_question)\n",
    "        logits = outputs['logits']\n",
    "        predicted_answer = torch.argmax(logits, dim=-1).item()\n",
    "        \n",
    "        # Also get top-3 predictions with probabilities\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        top_probs, top_indices = torch.topk(probs, k=3, dim=-1)\n",
    "        \n",
    "    model.train()\n",
    "    \n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Predicted answer: {predicted_answer}\")\n",
    "    print(\"Top 3 predictions:\")\n",
    "    for i in range(3):\n",
    "        ans = top_indices[0][i].item()\n",
    "        prob = top_probs[0][i].item()\n",
    "        print(f\"  {ans}: {prob:.3f}\")\n",
    "    \n",
    "    return predicted_answer\n",
    "\n",
    "# Test examples\n",
    "print(\"=== Interactive Testing ===\")\n",
    "ask_adder('2+3')\n",
    "print()\n",
    "ask_adder('5+4')\n",
    "print()\n",
    "ask_adder('9+9')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_analytics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
