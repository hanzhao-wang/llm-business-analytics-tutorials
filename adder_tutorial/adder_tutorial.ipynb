{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 2: GPT-Adder - Learning Arithmetic with Complete Input\n",
    "\n",
    "Welcome to the GPT-Adder tutorial! In this version, we train a transformer model to perform addition where:\n",
    "- **Input (X)**: Complete question like \"2+3=\"\n",
    "- **Output (Y)**: Single predicted answer like \"5\"\n",
    "\n",
    "This is different from the original autoregressive character-by-character prediction for NLP. Instead, we treat this as a sequence-to-single-token prediction task.\n",
    "\n",
    "**Goal:** Train a transformer to map complete addition questions to single numeric answers.\n",
    "\n",
    "- Input X is the full question \"a+b=\"\n",
    "- Output Y is a single token representing the answer\n",
    "- We'll use a classification approach where each possible answer is a class\n",
    "- Model architecture includes a classification head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Config, GPT2Model\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum possible answer: 198\n",
      "Number of answer classes: 199\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1182d2b70>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 32     # How many independent sequences will we process in parallel to speed up the training process\n",
    "max_iters = 4000       # Iteration of training \n",
    "eval_interval = 250    # Interval of evaluation\n",
    "learning_rate = 1e-3   # Learning rate for the optimizer\n",
    "device = 'cpu'         # The device to run the model on\n",
    "eval_iters = 100       # The number of iterations to evaluate the model\n",
    "n_embd = 128           # The number of embedding dimensions\n",
    "n_head = 4             # The number of attention heads\n",
    "n_layer = 4            # The number of layers\n",
    "dropout = 0.1          # Dropout rate, this is to prevent overfitting\n",
    "\n",
    "# Parameters for data generation\n",
    "ndigit = 2  # Up to 2-digit numbers (0-99)\n",
    "\n",
    "# Calculate maximum possible answer for classification\n",
    "max_answer = (10**ndigit - 1) + (10**ndigit - 1)  # e.g., 99+99=198 for ndigit=2\n",
    "num_answer_classes = max_answer + 1  # 0 to max_answer inclusive\n",
    "\n",
    "print(f\"Maximum possible answer: {max_answer}\")\n",
    "print(f\"Number of answer classes: {num_answer_classes}\")\n",
    "\n",
    "# For reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation: Maximum Possible Answer and Classification\n",
    "\n",
    "In this tutorial, we are framing the addition problem \"a+b=\" as a **classification task**. This means the model's goal is not to generate the sequence of digits for the answer, but to predict which *single class* the answer belongs to.\n",
    "\n",
    "Think of it like image classification where a model predicts if an image is a \"cat,\" \"dog,\" or \"bird.\" Here, our \"classes\" are all the possible numerical answers the addition problems can produce.\n",
    "\n",
    "1.  **Defining the Classes:**\n",
    "    Since our input numbers `a` and `b` are limited by `ndigit` (e.g., for `ndigit=2`, numbers range from 0 to 99), there's a maximum possible sum.\n",
    "    - If `ndigit=2`, the largest sum is 99 + 99 = 198.\n",
    "    - The smallest sum is 0 + 0 = 0.\n",
    "    So, all possible answers lie in the range \\[0, 198].\n",
    "\n",
    "2.  **`num_answer_classes`:**\n",
    "    Each unique integer answer in this range becomes a distinct \"class\" for our model.\n",
    "    - `max_answer = (10**ndigit - 1) + (10**ndigit - 1)` calculates this maximum sum.\n",
    "    - `num_answer_classes = max_answer + 1` determines the total number of unique classes (from 0 up to `max_answer`, inclusive). For `ndigit=2`, this is 198 + 1 = 199 classes.\n",
    "\n",
    "3.  **Why Classification?**\n",
    "    By treating this as a classification problem:\n",
    "    - The model's output layer (the \"classification head\") will have `num_answer_classes` neurons.\n",
    "    - Each neuron corresponds to one possible sum (e.g., neuron 0 for answer \"0\", neuron 1 for answer \"1\", ..., neuron 198 for answer \"198\").\n",
    "    - The model will output a probability distribution over these classes, and the class with the highest probability is chosen as the predicted answer.\n",
    "    - We use `CrossEntropyLoss`, which is standard for classification tasks.\n",
    "\n",
    "This approach simplifies the problem compared to generating an answer character by character, especially since the output (the sum) is a single entity. The model just needs to learn to map the input question sequence to the correct answer \"bucket\" or class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation\n",
    "\n",
    "### 3.1 Vocabulary and Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input vocabulary: '0123456789+= '\n",
      "Input vocabulary size: 13\n",
      "Original question: '2+3='\n",
      "Encoded: [2, 10, 3, 11]\n",
      "Decoded: '2+3='\n"
     ]
    }
   ],
   "source": [
    "input_chars = '0123456789+= '  # Added space at the end for padding\n",
    "input_vocab_size = len(input_chars)\n",
    "print(f\"Input vocabulary: '{input_chars}'\")\n",
    "print(f\"Input vocabulary size: {input_vocab_size}\")\n",
    "\n",
    "# Create mappings for input\n",
    "input_stoi = {ch: i for i, ch in enumerate(input_chars)} #mapping from input elements to index\n",
    "input_itos = {i: ch for i, ch in enumerate(input_chars)} #mapping from index to input elements\n",
    "\n",
    "def encode_input(s):\n",
    "    return [input_stoi[c] for c in s] #encode the input string into a list of indices\n",
    "\n",
    "def decode_input(l):\n",
    "    return ''.join([input_itos[i] for i in l]) #decode the list of indices into a string\n",
    "\n",
    "# Test encoding/decoding\n",
    "test_question = \"2+3=\"\n",
    "encoded_test = encode_input(test_question)\n",
    "decoded_test = decode_input(encoded_test)\n",
    "print(f\"Original question: '{test_question}'\")\n",
    "print(f\"Encoded: {encoded_test}\")\n",
    "print(f\"Decoded: '{decoded_test}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample problems:\n",
      "Question: '81+14=' -> Answer: 95\n",
      "Question: '3+94=' -> Answer: 97\n",
      "Question: '35+31=' -> Answer: 66\n",
      "Question: '28+17=' -> Answer: 45\n",
      "Question: '94+13=' -> Answer: 107\n"
     ]
    }
   ],
   "source": [
    "def generate_addition_data(num_digits):\n",
    "    \"\"\"Generate a single addition problem and answer.\"\"\"\n",
    "    a = random.randint(0, 10**num_digits - 1)\n",
    "    b = random.randint(0, 10**num_digits - 1)\n",
    "    c = a + b\n",
    "    question = f\"{a}+{b}=\"\n",
    "    answer = c  # Single integer answer\n",
    "    return question, answer\n",
    "\n",
    "# Test data generation\n",
    "print(\"Sample problems:\")\n",
    "for _ in range(5):\n",
    "    q, a = generate_addition_data(ndigit)\n",
    "    print(f\"Question: '{q}' -> Answer: {a}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum question length: 6\n",
      "Sample question: '86+94='\n",
      "Sample answer: '180'\n"
     ]
    }
   ],
   "source": [
    "# Calculate maximum question length for padding\n",
    "# For ndigit=1: max question is \"9+9=\" (4 characters)\n",
    "max_question_length = ndigit + 1 + ndigit + 1  # a + \"+\" + b + \"=\"\n",
    "print(f\"Maximum question length: {max_question_length}\")\n",
    "\n",
    "def get_batch():\n",
    "    \"\"\"\n",
    "    Generate a batch of padded addition problems. \n",
    "    \"\"\"\n",
    "    questions = []\n",
    "    answers = []\n",
    "    for _ in range(batch_size):\n",
    "        # Generate a new addition problem\n",
    "        q_str, ans_int = generate_addition_data(ndigit)\n",
    "        \n",
    "        # Pad and encode the question\n",
    "        # ljust(max_question_length): pad the question to the left with spaces to make it the same length as the maximum question length\n",
    "        # This is like what we do to fill the missing values in the dataset.\n",
    "        padded_q = q_str.ljust(max_question_length)\n",
    "        encoded_q = encode_input(padded_q)\n",
    "        \n",
    "        questions.append(encoded_q)\n",
    "        answers.append(ans_int)\n",
    "    \n",
    "    # Convert lists to tensors and move to the correct device\n",
    "    x = torch.tensor(questions, dtype=torch.long, device=device)\n",
    "    y = torch.tensor(answers, dtype=torch.long, device=device)\n",
    "\n",
    "    return x, y #shape: (batch_size, max_question_length)\n",
    "\n",
    "# Sample the first question and answer in the batch\n",
    "x,y = get_batch()\n",
    "print(f\"Sample question: '{decode_input(x[0].tolist())}'\")\n",
    "print(f\"Sample answer: '{y[0]}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation: Padding Input Sequences\n",
    "\n",
    "Transformer models, like the GPT-2 architecture we're using as a base, are designed to process sequences of a fixed length. However, our input questions (e.g., \"1+2=\", \"10+5=\", \"99+99=\") can have varying lengths.\n",
    "\n",
    "**Why Padding?**\n",
    "1.  **Batch Processing:** To train neural networks efficiently, we feed data in batches. All sequences within a single batch must have the same length so they can be processed in parallel by the GPU or CPU.\n",
    "2.  **Fixed-Size Model Input:** The transformer architecture itself expects inputs of a predefined maximum sequence length (`n_positions` in `GPT2Config`, which we set to `max_question_length`).\n",
    "\n",
    "This is just like in Sklearn, when we want to use rows with missing value, we need to first fill these missing values to make every row has the same length.\n",
    "\n",
    "**How Padding Works:**\n",
    "1.  **Determine `max_question_length`:** We first calculate the maximum possible length an input question can have. For `ndigit=2`, the longest question is \"99+99=\" (6 characters). This becomes our `max_question_length`.\n",
    "2.  **Add a Padding Token:** We add a special padding character to our input vocabulary (in this case, a space ' ').\n",
    "3.  **Pad Shorter Sequences:** Any question shorter than `max_question_length` is padded with this special character (usually at the end) until it reaches the `max_question_length`.\n",
    "    - \"2+3=\" (length 4) with `max_question_length=6` becomes \"2+3=  \" (length 6).\n",
    "    - The `ljust(max_question_len)` method handles this.\n",
    "\n",
    "**(Optional) Attention Mechanism and Padding:**\n",
    "While the input sequences are padded, the transformer's attention mechanism can be designed (often through an attention mask) to ignore these padding tokens during computation. This ensures that the padding doesn't negatively influence the learning process. For this specific `GPT2Model` from Hugging Face, it typically handles attention masking internally based on standard padding token IDs or by allowing explicit attention masks. In our simplified setup, the model will still \"see\" the padding tokens, but their embeddings will be learned like any other token. The key is that the *structure* of the input is now uniform across the batch.\n",
    "\n",
    "This padding ensures that all input tensors passed to the model have a consistent shape, which is essential for the underlying computations and batching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Definition\n",
    "\n",
    "We'll use GPT2Model (without the language modeling head) and add our own classification head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.82 M parameters\n"
     ]
    }
   ],
   "source": [
    "class AdditionClassifier(nn.Module):\n",
    "    \"\"\"Transformer model for addition classification.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_vocab_size, num_classes, max_seq_len, n_embd, n_layer, n_head, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        # GPT2 configuration for training the model from scratch\n",
    "        config = GPT2Config(\n",
    "            vocab_size=input_vocab_size,\n",
    "            n_positions=max_seq_len,\n",
    "            n_embd=n_embd,\n",
    "            n_layer=n_layer,\n",
    "            n_head=n_head,\n",
    "            resid_pdrop=dropout,\n",
    "            embd_pdrop=dropout,\n",
    "            attn_pdrop=dropout,\n",
    "            bos_token_id=None,\n",
    "            eos_token_id=None\n",
    "        )\n",
    "        \n",
    "        # Use GPT2Model (without LM head)\n",
    "        self.transformer = GPT2Model(config)\n",
    "        \n",
    "        # Classification head is a linear layer that maps the last hidden state to the number of classes (i.e. the number of possible answers from 0 to max_answer)\n",
    "        self.classifier = nn.Linear(n_embd, num_classes)\n",
    "     \n",
    "        \n",
    "    def forward(self, input_ids, labels=None):\n",
    "        # Get transformer outputs\n",
    "        transformer_outputs = self.transformer(input_ids)\n",
    "        hidden_states = transformer_outputs.last_hidden_state  # [batch_size, seq_len, n_embd]\n",
    "        \n",
    "        last_hidden = hidden_states[:, -1, :]  # [batch_size, n_embd]\n",
    "        #We can think the transformer is a feature extractor, and the last hidden state is the feature of the input. \n",
    "        #These features are like embeddings in the Word2vec/Net2vec.    \n",
    "        \n",
    "        # Apply classification head\n",
    "        logits = self.classifier(last_hidden)  # [batch_size, num_classes]\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            loss = loss_fn(logits, labels)\n",
    "        \n",
    "        return {'loss': loss, 'logits': logits}\n",
    "\n",
    "# Create model\n",
    "model = AdditionClassifier(\n",
    "    input_vocab_size=input_vocab_size,\n",
    "    num_classes=num_answer_classes,\n",
    "    max_seq_len=max_question_length,\n",
    "    n_embd=n_embd,\n",
    "    n_layer=n_layer,\n",
    "    n_head=n_head,\n",
    "    dropout=dropout\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "print(f\"{sum(p.numel() for p in model.parameters())/1e6:.2f} M parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation: Custom Classification Head\n",
    "\n",
    "The standard GPT-2 model from the `transformers` library, when used as `GPT2LMHeadModel`, is designed for **language modeling**. This means its primary goal is to predict the next token in a sequence, autoregressively. It has a \"language modeling head\" which is essentially a linear layer that maps the transformer's output hidden states to logits over the entire vocabulary (to predict the next word/character).\n",
    "\n",
    "**Our Task is Different:**\n",
    "In this tutorial, we are not performing traditional language modeling. Our task is **many-to-one classification**:\n",
    "-   **Input (Many):** A sequence of characters representing an addition problem (e.g., \"23+45=\").\n",
    "-   **Output (One):** A single class label representing the numerical answer (e.g., class 68).\n",
    "\n",
    "**Why `GPT2Model` + Custom Head?**\n",
    "\n",
    "1.  **Leveraging Transformer Power:** We still want to use the powerful sequence processing capabilities of the transformer architecture (self-attention, positional encodings, etc.) to understand the input question \"23+45=\". `GPT2Model` provides the core transformer blocks (embedding layer, multiple transformer layers) without the final language modeling layer.\n",
    "\n",
    "2.  **Tailoring to Classification:**\n",
    "    -   The output of `GPT2Model` is a sequence of hidden states, one for each input token. For our classification task, we are particularly interested in the information aggregated by the transformer over the entire sequence. A common strategy is to use the hidden state of the *last* token (or a special `[CLS]` token if one were used, or an aggregation like pooling). In our case, we use the hidden state corresponding to the final input token (which is often the '=' sign or a padding token if the actual question is shorter).\n",
    "    -   This chosen hidden state (a vector of size `n_embd`) is then fed into our custom **classification head**.\n",
    "\n",
    "3.  **The `self.classifier`:**\n",
    "    Our classification head is a simple `nn.Linear` layer: `self.classifier = nn.Linear(n_embd, num_classes)`.\n",
    "    -   It takes the `n_embd`-dimensional feature vector from the transformer.\n",
    "    -   It projects this vector into a `num_classes`-dimensional space. Each dimension in this output corresponds to one of the possible numerical answers (from 0 to `max_answer`).\n",
    "    -   The output of this linear layer are the **logits** for our classification task. Applying a softmax function to these logits gives the probabilities for each possible answer class.\n",
    "\n",
    "In summary, we use `GPT2Model` as a powerful feature extractor for our input sequence and then add a simple linear layer (`self.classifier`) on top to perform the final classification into one of the `num_answer_classes`. This adapts the general-purpose transformer architecture to our specific arithmetic task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer. We use AdamW as the optimizer like nanoGPT.\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = []\n",
    "        accuracies = []\n",
    "        for _ in range(eval_iters):\n",
    "            # Generate a batch of data\n",
    "            X, Y = get_batch()\n",
    "            \n",
    "            outputs = model(X, labels=Y)\n",
    "            loss = outputs['loss']\n",
    "            logits = outputs['logits']\n",
    "            \n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            accuracy = (predictions == Y).float().mean().item()\n",
    "            accuracies.append(accuracy)\n",
    "            \n",
    "        out[split + '_loss'] = np.mean(losses)\n",
    "        out[split + '_acc'] = np.mean(accuracies)\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cpu...\n",
      "step 0: train loss 5.5229, train acc 0.0047, val loss 5.5278, val acc 0.0050\n",
      "step 250: train loss 3.9133, train acc 0.0484, val loss 3.9162, val acc 0.0453\n",
      "step 500: train loss 3.3892, train acc 0.0816, val loss 3.3954, val acc 0.0659\n",
      "step 750: train loss 3.2182, train acc 0.0659, val loss 3.2197, val acc 0.0684\n",
      "step 1000: train loss 3.0144, train acc 0.0984, val loss 2.9948, val acc 0.1031\n",
      "step 1250: train loss 2.9407, train acc 0.0963, val loss 2.9368, val acc 0.0891\n",
      "step 1500: train loss 2.7494, train acc 0.1259, val loss 2.7651, val acc 0.1050\n",
      "step 1750: train loss 2.7110, train acc 0.1187, val loss 2.7045, val acc 0.1313\n",
      "step 2000: train loss 2.6228, train acc 0.1228, val loss 2.6257, val acc 0.1259\n",
      "step 2250: train loss 2.6725, train acc 0.1100, val loss 2.6598, val acc 0.1122\n",
      "step 2500: train loss 2.4730, train acc 0.1600, val loss 2.4809, val acc 0.1603\n",
      "step 2750: train loss 2.3953, train acc 0.1669, val loss 2.4054, val acc 0.1550\n",
      "step 3000: train loss 2.3930, train acc 0.1606, val loss 2.3902, val acc 0.1672\n",
      "step 3250: train loss 2.3353, train acc 0.1756, val loss 2.3226, val acc 0.1750\n",
      "step 3500: train loss 2.3306, train acc 0.1762, val loss 2.3389, val acc 0.1744\n",
      "step 3750: train loss 2.2138, train acc 0.1894, val loss 2.2396, val acc 0.1837\n",
      "step 3999: train loss 2.2737, train acc 0.1819, val loss 2.2918, val acc 0.1716\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "print(f\"Training on {device}...\")\n",
    "\n",
    "for iter_num in range(max_iters):\n",
    "    # Evaluation part\n",
    "    if iter_num % eval_interval == 0 or iter_num == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter_num}: train loss {losses.get('train_loss', float('nan')):.4f}, train acc {losses.get('train_acc', float('nan')):.4f}, val loss {losses.get('val_loss', float('nan')):.4f}, val acc {losses.get('val_acc', float('nan')):.4f}\")\n",
    "\n",
    "    # Training part\n",
    "\n",
    "    # 1. Get a batch of data\n",
    "    questions, answers = get_batch()\n",
    "\n",
    "    # 2. Forward pass\n",
    "    outputs = model(questions, labels=answers)\n",
    "    \n",
    "    # 3. Calculate loss\n",
    "    loss = outputs['loss']\n",
    "\n",
    "    # 4 and 5. Backward pass and update parameters\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Testing and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Testing model on 20 examples (up to 2-digit numbers) ---\n",
      "Problem  1: 3+54=57 -> Model predicted: 56 -> INCORRECT\n",
      "Problem  2: 40+52=92 -> Model predicted: 92 -> CORRECT\n",
      "Problem  3: 83+85=168 -> Model predicted: 168 -> CORRECT\n",
      "Problem  4: 94+11=105 -> Model predicted: 106 -> INCORRECT\n",
      "Problem  5: 67+95=162 -> Model predicted: 162 -> CORRECT\n",
      "Problem  6: 25+72=97 -> Model predicted: 99 -> INCORRECT\n",
      "Problem  7: 60+99=159 -> Model predicted: 162 -> INCORRECT\n",
      "Problem  8: 67+2=69 -> Model predicted: 66 -> INCORRECT\n",
      "Problem  9: 20+44=64 -> Model predicted: 66 -> INCORRECT\n",
      "Problem 10: 77+4=81 -> Model predicted: 79 -> INCORRECT\n",
      "Problem 11: 81+26=107 -> Model predicted: 106 -> INCORRECT\n",
      "Problem 12: 23+15=38 -> Model predicted: 40 -> INCORRECT\n",
      "Problem 13: 17+31=48 -> Model predicted: 48 -> CORRECT\n",
      "Problem 14: 68+36=104 -> Model predicted: 101 -> INCORRECT\n",
      "Problem 15: 64+84=148 -> Model predicted: 147 -> INCORRECT\n",
      "Problem 16: 28+89=117 -> Model predicted: 115 -> INCORRECT\n",
      "Problem 17: 26+89=115 -> Model predicted: 113 -> INCORRECT\n",
      "Problem 18: 99+15=114 -> Model predicted: 113 -> INCORRECT\n",
      "Problem 19: 31+98=129 -> Model predicted: 128 -> INCORRECT\n",
      "Problem 20: 54+42=96 -> Model predicted: 96 -> CORRECT\n",
      "Accuracy: 25.00% (5/20 correct)\n"
     ]
    }
   ],
   "source": [
    "def test_model_addition(num_tests=20, num_digits_test=ndigit):\n",
    "    print(f\"--- Testing model on {num_tests} examples (up to {num_digits_test}-digit numbers) ---\")\n",
    "    model.eval()\n",
    "    correct_predictions = 0\n",
    "\n",
    "    for i in range(num_tests):\n",
    "        # Generate test problem\n",
    "        a = random.randint(0, 10**num_digits_test - 1)\n",
    "        b = random.randint(0, 10**num_digits_test - 1)\n",
    "        correct_answer = a + b\n",
    "        question = f\"{a}+{b}=\"\n",
    "        \n",
    "        # Pad and encode question\n",
    "        padded_question = question.ljust(max_question_length)\n",
    "        encoded_question = torch.tensor(encode_input(padded_question), dtype=torch.long, device=device).unsqueeze(0)\n",
    "        \n",
    "        # Get model prediction\n",
    "        with torch.no_grad():\n",
    "            outputs = model(encoded_question)\n",
    "            logits = outputs['logits']\n",
    "            predicted_answer = torch.argmax(logits, dim=-1).item()\n",
    "        \n",
    "        is_correct = (predicted_answer == correct_answer)\n",
    "        if is_correct:\n",
    "            correct_predictions += 1\n",
    "            status = \"CORRECT\"\n",
    "        else:\n",
    "            status = \"INCORRECT\"\n",
    "        \n",
    "        print(f\"Problem {i+1:2d}: {question}{correct_answer} -> Model predicted: {predicted_answer} -> {status}\")\n",
    "    \n",
    "    accuracy = (correct_predictions / num_tests) * 100\n",
    "    print(f\"Accuracy: {accuracy:.2f}% ({correct_predictions}/{num_tests} correct)\")\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "# Run test\n",
    "test_model_addition(num_tests=20, num_digits_test=ndigit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Interactive Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Interactive Testing ===\n",
      "Question: 2+3=\n",
      "Predicted answer: 5\n",
      "Top 3 predictions:\n",
      "  5: 0.134\n",
      "  1: 0.133\n",
      "  8: 0.110\n",
      "\n",
      "Question: 5+4=\n",
      "Predicted answer: 8\n",
      "Top 3 predictions:\n",
      "  8: 0.137\n",
      "  10: 0.137\n",
      "  7: 0.097\n",
      "\n",
      "Question: 9+9=\n",
      "Predicted answer: 1\n",
      "Top 3 predictions:\n",
      "  1: 0.270\n",
      "  8: 0.071\n",
      "  12: 0.064\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ask_adder(problem_input):\n",
    "    \"\"\"Ask the model to solve an addition problem.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Ensure input ends with '='\n",
    "    if not problem_input.endswith('='):\n",
    "        question = problem_input + '='\n",
    "    else:\n",
    "        question = problem_input\n",
    "    \n",
    "    # Pad and encode\n",
    "    padded_question = question.ljust(max_question_length)\n",
    "    encoded_question = torch.tensor(encode_input(padded_question), dtype=torch.long, device=device).unsqueeze(0)\n",
    "    \n",
    "    # Get prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model(encoded_question)\n",
    "        logits = outputs['logits']\n",
    "        predicted_answer = torch.argmax(logits, dim=-1).item()\n",
    "        \n",
    "        # Also get top-3 predictions with probabilities\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        top_probs, top_indices = torch.topk(probs, k=3, dim=-1)\n",
    "        \n",
    "    model.train()\n",
    "    \n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Predicted answer: {predicted_answer}\")\n",
    "    print(\"Top 3 predictions:\")\n",
    "    for i in range(3):\n",
    "        ans = top_indices[0][i].item()\n",
    "        prob = top_probs[0][i].item()\n",
    "        print(f\"  {ans}: {prob:.3f}\")\n",
    "    \n",
    "    return predicted_answer\n",
    "\n",
    "# Test examples\n",
    "print(\"=== Interactive Testing ===\")\n",
    "ask_adder('2+3')\n",
    "print()\n",
    "ask_adder('5+4')\n",
    "print()\n",
    "ask_adder('9+9')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GenAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
