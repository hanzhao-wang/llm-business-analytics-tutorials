{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1: nanoGPT - Word-Level Text Generation\n",
    "\n",
    "Welcome to the nanoGPT tutorial! In this notebook, we will train a simple word-level language model to generate text.\n",
    "\n",
    "**Goal:** Understand how to train a small transformer model for text generation on a CPU.\n",
    "\n",
    "**Key Concepts:**\n",
    "- Word-level tokenization\n",
    "- Preparing data for language modeling\n",
    "- Building a GPT-style transformer from scratch (simplified)\n",
    "- A basic PyTorch training loop\n",
    "- Generating text with the trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "First, let's import the necessary libraries. We'll need `torch` for building and training our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x110cd2b90>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from transformers import GPT2Config, GPT2LMHeadModel\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# For reproducibility\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "We'll define some hyperparameters for our model and training process. Since we're running on CPU and want this to be quick for a tutorial, these values will be small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 32       # How many independent sequences will we process in parallel \n",
    "block_size = 10      # What is the maximum context length for predictions?\n",
    "max_iters = 5000      # Total training iterations\n",
    "eval_interval = 500   # How often to evaluate on validation set\n",
    "learning_rate = 5e-4  # Learning rate for the optimizer\n",
    "device = 'cpu'        # Explicitly set to CPU\n",
    "eval_iters = 100      # Number of iterations for evaluation\n",
    "n_embd = 128          # Embedding dimension (reduced for CPU)\n",
    "n_head = 4            # Number of attention heads (reduced for CPU)\n",
    "n_layer = 4           # Number of transformer blocks (reduced for CPU)\n",
    "dropout = 0.0          # Dropout rate>0 to prevent overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation\n",
    "\n",
    "We'll use a small text file as our dataset. For this tutorial, let's use a snippet of Shakespeare's writings.\n",
    "\n",
    "### 3.1 Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use a small snippet of text for this tutorial.\n",
    "# You can replace this with the path to a larger .txt file if you wish.\n",
    "text = \"\"\"To stale 't a little more.\n",
    "\n",
    "First Citizen:\n",
    "Well, I'll hear it, sir: yet you must not think to\n",
    "fob off our disgrace with a tale: but, an 't please\n",
    "you, deliver.\n",
    "\n",
    "MENENIUS:\n",
    "There was a time when all the body's members\n",
    "Rebell'd against the belly, thus accused it:\n",
    "That only like a gulf it did remain\n",
    "I' the midst o' the body, idle and unactive,\n",
    "Still cupboarding the viand, never bearing\n",
    "Like labour with the rest, where the other instruments\n",
    "Did see and hear, devise, instruct, walk, feel,\n",
    "And, mutually participate, did minister\n",
    "Unto the appetite and affection common\n",
    "Of the whole body. The belly answer'd--\n",
    "\n",
    "First Citizen:\n",
    "Well, sir, what answer made the belly?\n",
    "\n",
    "MENENIUS:\n",
    "Sir, I shall tell you. With a kind of smile,\n",
    "Which ne'er came from the lungs, but even thus--\n",
    "For, look you, I may make the belly smile\n",
    "As well as speak--it tauntingly replied\n",
    "To the discontented members, the mutinous parts\n",
    "That envied his receipt; even so most fitly\n",
    "As you malign our senators for that\n",
    "They are not such as you.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Word-level Tokenization\n",
    "\n",
    "Since this is a word-level model, our vocabulary will consist of all unique words present in the text. We'll create mappings from words to integers (encode) and integers to words (decode)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample words: [\"'\", ',', '-', '.', ':', ';', '?', 'a', 'accused', 'affection', 'against', 'all', 'an', 'and', 'answer', 'appetite', 'are', 'as', 'bearing', 'belly']\n",
      "Vocabulary size: 123\n",
      "Original: an answer\n",
      "Encoded: [12, 14]\n",
      "Decoded: an answer\n"
     ]
    }
   ],
   "source": [
    "# Get all unique words in the text by regular expression\n",
    "import re \n",
    "\n",
    "# Simple word tokenization - split on whitespace and punctuation\n",
    "words = re.findall(r'\\b\\w+\\b|[^\\w\\s]', text.lower())\n",
    "unique_words = sorted(list(set(words)))\n",
    "vocab_size = len(unique_words)\n",
    "\n",
    "print(\"Sample words:\", unique_words[:20])\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "\n",
    "# Create a mapping from words to integers and vice-versa\n",
    "stoi = {word: i for i, word in enumerate(unique_words)}\n",
    "itos = {i: word for i, word in enumerate(unique_words)}\n",
    "\n",
    "def encode(s):\n",
    "    \"\"\"Encoder: take a string, output a list of integers\"\"\"\n",
    "    words = re.findall(r'\\b\\w+\\b|[^\\w\\s]', s.lower())\n",
    "    return [stoi[word] for word in words if word in stoi]\n",
    "\n",
    "def decode(l):\n",
    "    \"\"\"Decoder: take a list of integers, output a string\"\"\"\n",
    "    words = [itos[i] for i in l]\n",
    "    # Simple reconstruction - join with spaces, handle punctuation\n",
    "    result = \"\"\n",
    "    for i, word in enumerate(words):\n",
    "        if word in \".,!?;:\":\n",
    "            result += word\n",
    "        elif i == 0:\n",
    "            result += word\n",
    "        else:\n",
    "            result += \" \" + word\n",
    "    return result\n",
    "\n",
    "# Test encoding and decoding\n",
    "test_text = \"an answer\"\n",
    "encoded = encode(test_text)\n",
    "decoded = decode(encoded)\n",
    "print(f\"Original: {test_text}\")\n",
    "print(f\"Encoded: {encoded}\")\n",
    "print(f\"Decoded: {decoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation: Tokenizer and Tokens\n",
    "\n",
    "A **tokenizer** is a crucial component in Natural Language Processing (NLP). Its primary job is to break down a piece of text (like a sentence or a paragraph) into smaller units called **tokens**. These tokens can be words, sub-words, or even individual characters, depending on the type of tokenizer used.\n",
    "\n",
    "Think of it like this: computers don't understand words and sentences directly. They understand numbers. So, a tokenizer first segments the text and then, in conjunction with a vocabulary mapping (like our `stoi` and `itos` dictionaries), converts these tokens into numerical representations that a machine learning model can process.\n",
    "\n",
    "In this notebook, we are using a **word-level tokenizer**. This means each token is a unique word or punctuation mark found in our text.\n",
    "- The `re.findall(r'\\b\\w+\\b|[^\\w\\s]', text.lower())` line is our simple tokenizer. It uses regular expressions to find sequences of word characters (`\\w+`) or any character that is not a word character or whitespace (`[^\\w\\s]`). This helps us capture both words and punctuation marks as separate tokens.\n",
    "- `stoi` (string to integer) maps each unique token (word/punctuation) to a unique integer.\n",
    "- `itos` (integer to string) does the reverse, mapping integers back to their original tokens.\n",
    "\n",
    "This process is fundamental because it allows the model to learn patterns and relationships between these numerical representations of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Create Training and Validation Splits\n",
    "\n",
    "We'll split our dataset into a training set and a validation set. The model learns from the training set, and we use the validation set to check how well it's generalizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: torch.Size([238]), Data type: torch.int64\n",
      "Sample encoded text: [108, 94, 0, 97, 7, 52, 64, 3, 36, 23, 4, 114, 1, 44, 0, 53, 42, 48, 1, 90]\n",
      "Sample decoded text: to stale ' t a little more. first citizen: well, i ' ll hear it, sir\n",
      "Training data length: 119 words\n",
      "Validation data length: 119 words\n"
     ]
    }
   ],
   "source": [
    "# Encode the entire text dataset and store it into a torch.Tensor\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(f\"Data shape: {data.shape}, Data type: {data.dtype}\")\n",
    "print(f\"Sample encoded text: {data[:20].tolist()}\")\n",
    "print(f\"Sample decoded text: {decode(data[:20].tolist())}\")\n",
    "\n",
    "# Split up the data into train and validation sets\n",
    "n = int(0.5*len(data)) # first 50% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "print(f\"Training data length: {len(train_data)} words\")\n",
    "print(f\"Validation data length: {len(val_data)} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Data Loader\n",
    "\n",
    "We need a way to feed data to our model in batches. The `get_batch` function will randomly sample `batch_size` chunks of `block_size` length from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([32, 10])\n",
      "targets:\n",
      "torch.Size([32, 10])\n",
      "----\n",
      "Word-level training examples:\n",
      "Context: 'instruments' -> Target: 'did'\n",
      "Context: 'instruments did' -> Target: 'see'\n",
      "Context: 'instruments did see' -> Target: 'and'\n",
      "Context: 'instruments did see and' -> Target: 'hear'\n",
      "Context: 'instruments did see and hear' -> Target: ','\n",
      "Context: 'instruments did see and hear,' -> Target: 'devise'\n",
      "Context: 'instruments did see and hear, devise' -> Target: ','\n",
      "Context: 'instruments did see and hear, devise,' -> Target: 'instruct'\n",
      "\n",
      "Context: 'only' -> Target: 'like'\n",
      "Context: 'only like' -> Target: 'a'\n",
      "Context: 'only like a' -> Target: 'gulf'\n",
      "Context: 'only like a gulf' -> Target: 'it'\n",
      "Context: 'only like a gulf it' -> Target: 'did'\n",
      "Context: 'only like a gulf it did' -> Target: 'remain'\n",
      "Context: 'only like a gulf it did remain' -> Target: 'i'\n",
      "Context: 'only like a gulf it did remain i' -> Target: '''\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  Data loading function \n",
    "def get_batch(split):\n",
    "    # Generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "# Example of a batch\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "\n",
    "print('----')\n",
    "print(\"Word-level training examples:\")\n",
    "for b in range(min(2, batch_size)):  # Show 2 examples\n",
    "    for t in range(min(8, block_size)):  # Show first 8 positions\n",
    "        context_words = decode(xb[b, :t+1].tolist())\n",
    "        target_word = itos[yb[b,t].item()]\n",
    "        print(f\"Context: '{context_words}' -> Target: '{target_word}'\")\n",
    "        if t == 7:  # Only show first 8 examples\n",
    "            break\n",
    "    print()\n",
    "    if b == 1:  # Only show for first 2 batch items\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation: `get_batch` - Why X and Y have the same length\n",
    "\n",
    "In the `get_batch` function, both the input `x` and the target `y` are sequences of `block_size` length. This might seem counterintuitive at first, as we are trying to predict the *next* word. Let's clarify:\n",
    "\n",
    "The task of a language model is to predict the next token in a sequence, given the preceding tokens.\n",
    "- `x` represents the **input context**: a sequence of words the model sees.\n",
    "- `y` represents the **target output**: for each position in `x`, `y` contains the word that *immediately follows* it in the original text.\n",
    "\n",
    "Consider a `block_size` of 10.\n",
    "If `x` is `[word_1, word_2, ..., word_10]`,\n",
    "then `y` will be `[word_2, word_3, ..., word_11]`.\n",
    "\n",
    "So, for each element `x[i]` in the input sequence (at a specific time step `t` within the block), the corresponding `y[i]` is the actual next word that the model should have predicted.\n",
    "\n",
    "For example, when the model sees:\n",
    "- `x[0]` (i.e., `word_1`), it tries to predict `y[0]` (i.e., `word_2`).\n",
    "- `x[0:1]` (i.e., `word_1, word_2`), it tries to predict `y[1]` (i.e., `word_3`).\n",
    "- ...\n",
    "- `x[0:9]` (i.e., `word_1, ..., word_10`), it tries to predict `y[9]` (i.e., `word_11`).\n",
    "\n",
    "The model makes predictions for *every position* in the `block_size`. The loss function then compares all these predictions with the actual next words in `y`. This way, a single block of data provides `block_size` individual training examples for the model to learn from. The \"context\" for predicting `y[t]` is `x[0...t]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Definition (Simplified nanoGPT Style)\n",
    "\n",
    "Now we'll build a simplified version of the GPT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.81 M parameters\n"
     ]
    }
   ],
   "source": [
    "# Define the GPT-2 model configuration using transformers library\n",
    "config = GPT2Config(\n",
    "    vocab_size=vocab_size,\n",
    "    n_positions=block_size,  # Max sequence length\n",
    "    n_embd=n_embd,\n",
    "    n_layer=n_layer,\n",
    "    n_head=n_head,\n",
    "    resid_pdrop=dropout,\n",
    "    embd_pdrop=dropout,\n",
    "    attn_pdrop=dropout,\n",
    "    bos_token_id=None,  # No beginning of sentence token for word model\n",
    "    eos_token_id=None   # No end of sentence token for word model\n",
    ")\n",
    "\n",
    "# Instantiate the model\n",
    "model = GPT2LMHeadModel(config)\n",
    "model.to(device)\n",
    "\n",
    "# Print the number of parameters in the model\n",
    "print(f\"{sum(p.numel() for p in model.parameters())/1e6:.2f} M parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training the Model\n",
    "\n",
    "### 5.1 Optimizer\n",
    "We'll use the AdamW optimizer, a common choice for training transformer models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Loss Estimation\n",
    "We need a function to estimate the loss on the training and validation sets without calculating gradients, which is useful for monitoring training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad() # Decorator to disable gradient calculation\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            outputs = model(X, labels=Y)\n",
    "            loss = outputs.loss\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train() # Set model back to training mode\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Training Loop\n",
    "This is the main loop where the model learns. For each iteration, we:\n",
    "1. Get a batch of data.\n",
    "2. Perform a forward pass (get model predictions).\n",
    "3. Calculate the loss (how wrong the predictions are).\n",
    "4. Perform a backward pass (calculate gradients).\n",
    "5. Update the model's parameters using the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cpu...\n",
      "step 0: train loss 4.8432, val loss 4.8423\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m xb, yb = get_batch(\u001b[33m'\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Evaluate the loss\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43myb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m loss = outputs.loss\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Backward pass and optimization step\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/llm-business-analytics-tutorials-main/GenAI/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/llm-business-analytics-tutorials-main/GenAI/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/llm-business-analytics-tutorials-main/GenAI/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:1210\u001b[39m, in \u001b[36mGPT2LMHeadModel.forward\u001b[39m\u001b[34m(self, input_ids, past_key_values, cache_position, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[39m\n\u001b[32m   1190\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1191\u001b[39m \u001b[33;03minput_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\u001b[39;00m\n\u001b[32m   1192\u001b[39m \u001b[33;03m    `input_ids_length` = `sequence_length` if `past_key_values` is `None` else\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1206\u001b[39m \u001b[33;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[32m   1207\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1208\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m-> \u001b[39m\u001b[32m1210\u001b[39m transformer_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1211\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1212\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1213\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1214\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1215\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1216\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1217\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1218\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1219\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1220\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1221\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1222\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1223\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1224\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1225\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1226\u001b[39m hidden_states = transformer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1228\u001b[39m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/llm-business-analytics-tutorials-main/GenAI/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/llm-business-analytics-tutorials-main/GenAI/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/llm-business-analytics-tutorials-main/GenAI/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:939\u001b[39m, in \u001b[36mGPT2Model.forward\u001b[39m\u001b[34m(self, input_ids, past_key_values, cache_position, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[39m\n\u001b[32m    926\u001b[39m     outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m    927\u001b[39m         block.\u001b[34m__call__\u001b[39m,\n\u001b[32m    928\u001b[39m         hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    936\u001b[39m         output_attentions,\n\u001b[32m    937\u001b[39m     )\n\u001b[32m    938\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m939\u001b[39m     outputs = \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    940\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    952\u001b[39m hidden_states = outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    954\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/llm-business-analytics-tutorials-main/GenAI/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/llm-business-analytics-tutorials-main/GenAI/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/llm-business-analytics-tutorials-main/GenAI/lib/python3.11/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/llm-business-analytics-tutorials-main/GenAI/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:439\u001b[39m, in \u001b[36mGPT2Block.forward\u001b[39m\u001b[34m(self, hidden_states, past_key_value, cache_position, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, **kwargs)\u001b[39m\n\u001b[32m    437\u001b[39m residual = hidden_states\n\u001b[32m    438\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.ln_2(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m439\u001b[39m feed_forward_hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    440\u001b[39m \u001b[38;5;66;03m# residual connection\u001b[39;00m\n\u001b[32m    441\u001b[39m hidden_states = residual + feed_forward_hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/llm-business-analytics-tutorials-main/GenAI/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/llm-business-analytics-tutorials-main/GenAI/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/llm-business-analytics-tutorials-main/GenAI/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:365\u001b[39m, in \u001b[36mGPT2MLP.forward\u001b[39m\u001b[34m(self, hidden_states)\u001b[39m\n\u001b[32m    363\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: Optional[Tuple[torch.FloatTensor]]) -> torch.FloatTensor:\n\u001b[32m    364\u001b[39m     hidden_states = \u001b[38;5;28mself\u001b[39m.c_fc(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m365\u001b[39m     hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    366\u001b[39m     hidden_states = \u001b[38;5;28mself\u001b[39m.c_proj(hidden_states)\n\u001b[32m    367\u001b[39m     hidden_states = \u001b[38;5;28mself\u001b[39m.dropout(hidden_states)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/llm-business-analytics-tutorials-main/GenAI/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/llm-business-analytics-tutorials-main/GenAI/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "print(f\"Training on {device}...\")\n",
    "\n",
    "for iter_num in range(max_iters):\n",
    "\n",
    "    # Every once in a while, evaluate the loss on train and val sets\n",
    "    if iter_num % eval_interval == 0 or iter_num == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # Sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # Evaluate the loss\n",
    "    outputs = model(xb, labels=yb)\n",
    "    loss = outputs.loss\n",
    "    \n",
    "    # Backward pass and optimization step\n",
    "    optimizer.zero_grad(set_to_none=True) # Zero out gradients from previous step\n",
    "    loss.backward() # Compute gradients for the current batch\n",
    "    optimizer.step() # Update model parameters\n",
    "\n",
    "print(\"Training finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation: The Overfitting Issue\n",
    "\n",
    "**Overfitting** is a common problem in machine learning. It occurs when a model learns the training data *too well*, including its noise and specific details, to the point where it performs poorly on new, unseen data (like our validation set or real-world data).\n",
    "\n",
    "Imagine a student who memorizes the answers to all the questions in a specific textbook (the training data) but doesn't understand the underlying concepts. When given a new exam with slightly different questions (the validation or test data), the student would likely fail. This is analogous to an overfitted model.\n",
    "\n",
    "In our case, since the training data (the first part of the text) is very different from the testing data (the second part), the LLM cannot learn to predict the testing data from training. \n",
    "\n",
    "**Signs of Overfitting:**\n",
    "-   **Training loss continues to decrease, while validation loss starts to increase or plateaus.** This is a classic indicator. It means the model is getting better at fitting the training data but worse (or no better) at generalizing to unseen data.\n",
    "-   The model might produce excellent results on examples it has seen during training but generate nonsensical or poor outputs for new inputs.\n",
    "\n",
    "**(Optianal) How `dropout` helps (as a regularization technique):**\n",
    "Dropout is a simple yet effective regularization technique to combat overfitting in neural networks. Here's how it works during training:\n",
    "1.  At each training step, for every neuron in a layer where dropout is applied, it is \"dropped out\" (i.e., temporarily removed from the network) with a certain probability (`dropout` rate).\n",
    "2.  This means the neuron, along with all its incoming and outgoing connections, is ignored during the forward pass and backward pass for that particular training iteration.\n",
    "3.  The choice of which neurons to drop is random for each training iteration and each input example.\n",
    "\n",
    "**Why does this help?**\n",
    "-   **Prevents Co-adaptation:** Neurons become less reliant on specific other neurons because they can't be sure which ones will be active. This encourages each neuron to learn more robust features that are useful in conjunction with different random subsets of other neurons.\n",
    "-   **Ensemble Effect (Implicit):** Training with dropout can be seen as training a large number of thinned networks (networks with different subsets of neurons). At test time (when dropout is turned off), using the full network can be viewed as an approximation of averaging the predictions of these many thinned networks.\n",
    "\n",
    "In our configuration, we have `dropout = 0.0`. This means dropout is currently turned off. If you observe overfitting (e.g., validation loss increasing significantly while training loss is very low), you could try increasing the `dropout` rate (e.g., to 0.1 or 0.2) to see if it improves generalization. Note that dropout is typically applied during training only and is turned off during evaluation or inference (which `model.eval()` and `model.train()` handle for relevant layers in PyTorch)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generating Text\n",
    "\n",
    "Now that our model is trained, let's use it to generate some text. We'll start with a context (e.g., a newline word) and ask the model to predict the next words sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text...\n",
      "Starting context: 'sir: yet you must not think to fob off our disgrace with a tale'\n",
      "--- Generated Text ---\n",
      "sir: yet you must not think to fob off our disgrace with a tale but an t you deliver menenius there a more first: was time all body s rebell d the, ' against belly thus it remain ' midst ' accused:, accused:, ' hear,,,,,,,,: was ' hear,\n",
      "--- Original Text ---\n",
      "First Citizen:\n",
      "Well, I'll hear it, sir: yet you must not think to\n",
      "fob off our disgrace with a tale: but, an 't please\n",
      "you, deliver.\n",
      "\n",
      "MENENIUS:\n",
      "There was a time when all the body's members\n",
      "Rebell'd against the belly, thus accused it:\n",
      "That only like a gulf it did remain\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "# Generation function - create a simple generation loop\n",
    "print(\"Generating text...\")\n",
    "model.eval()\n",
    "\n",
    "# Start with the first few words from our training data\n",
    "start_words = block_size\n",
    "text_test=\" \".join(decode(train_data[:start_words].tolist()).split()[:start_words])\n",
    "text_test=\"sir: yet you must not think to fob off our disgrace with a tale\"\n",
    "context = torch.tensor(encode(text_test), \n",
    "                      dtype=torch.long, device=device).unsqueeze(0)\n",
    "\n",
    "print(f\"Starting context: '{decode(context[0].tolist())}'\")\n",
    "\n",
    "# Simple generation loop\n",
    "generated = context\n",
    "with torch.no_grad():\n",
    "    for _ in range(50):  # Generate 50 more words\n",
    "        # Get predictions for the current context\n",
    "        # Crop to last block_size tokens if context gets too long\n",
    "        context_cropped = generated[:, -block_size:] if generated.size(1) > block_size else generated\n",
    "        \n",
    "        outputs = model(context_cropped)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Get the logits for the last token\n",
    "        logits = logits[:, -1, :]  # (batch_size, vocab_size)\n",
    "        \n",
    "        # Apply temperature for sampling\n",
    "        temperature = 1\n",
    "        logits = logits / temperature\n",
    "        \n",
    "        # Sample from the distribution\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        \n",
    "        # Append to generated sequence\n",
    "        generated = torch.cat([generated, next_token], dim=1)\n",
    "\n",
    "# Decode the generated text\n",
    "generated_text = decode(generated[0].tolist())\n",
    "print(\"--- Generated Text ---\")\n",
    "print(generated_text)\n",
    "print(\"--- Original Text ---\")\n",
    "print('''First Citizen:\n",
    "Well, I'll hear it, sir: yet you must not think to\n",
    "fob off our disgrace with a tale: but, an 't please\n",
    "you, deliver.\n",
    "\n",
    "MENENIUS:\n",
    "There was a time when all the body's members\n",
    "Rebell'd against the belly, thus accused it:\n",
    "That only like a gulf it did remain''')\n",
    "print(\"----------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation: Temperature and Sampling in Text Generation\n",
    "\n",
    "When our model generates text, it doesn't just pick the single most probable next word. Instead, it often uses a technique called **sampling**, sometimes influenced by a parameter called **temperature**.\n",
    "\n",
    "1.  **Logits and Probabilities:**\n",
    "    After processing the input context, the model outputs **logits**. These are raw, unnormalized scores for each word in the vocabulary. To turn these logits into probabilities (i.e., values between 0 and 1 that sum up to 1), we apply a **softmax function**. The word with the highest probability is the one the model thinks is most likely to come next.\n",
    "\n",
    "2.  **Sampling:**\n",
    "    Instead of always picking the word with the absolute highest probability (which is called greedy decoding and can lead to repetitive or boring text), we can *sample* from the probability distribution. This means a word that is slightly less probable might still be chosen, introducing randomness and creativity into the generated text.\n",
    "    `torch.multinomial(probs, num_samples=1)` is the function that performs this sampling. It takes the probability distribution (`probs`) and picks one word based on these probabilities.\n",
    "\n",
    "3.  **Temperature:**\n",
    "    Temperature is a hyperparameter that controls the randomness of the sampling. It's applied to the logits *before* the softmax function.\n",
    "    -   **Low Temperature (e.g., < 1.0, closer to 0):** Dividing logits by a small number makes the differences between them larger. When softmax is applied, the probability distribution becomes \"sharper\" or \"spikier.\" The model becomes more confident and deterministic, tending to pick the most likely words. This can lead to more focused and coherent text, but potentially less creative.\n",
    "    -   **High Temperature (e.g., > 1.0):** Dividing logits by a larger number makes the differences between them smaller. The resulting softmax probability distribution becomes \"flatter\" or \"softer.\" The model becomes less confident, and less likely words have a higher chance of being selected. This increases randomness and creativity, but can also lead to more errors or nonsensical text.\n",
    "    -   **Temperature = 1.0:** This is the standard setting where the original probabilities are used for sampling.\n",
    "\n",
    "In our generation code:\n",
    "`logits = logits / temperature`\n",
    "`probs = F.softmax(logits, dim=-1)`\n",
    "\n",
    "By adjusting the `temperature` value, you can control the trade-off between coherence and creativity in the generated text. For this tutorial, we use a temperature of 1.0, which means we sample directly from the model's learned probability distribution."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GenAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
