{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1: nanoGPT - Word-Level Text Generation\n",
    "\n",
    "Welcome to the nanoGPT tutorial! In this notebook, we will train a simple word-level language model to generate text.\n",
    "\n",
    "**Goal:** Understand how to train a small transformer model for text generation on a CPU.\n",
    "\n",
    "**Key Concepts:**\n",
    "- Word-level tokenization\n",
    "- Preparing data for language modeling\n",
    "- A basic PyTorch training loop\n",
    "- Generating text with the trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "First, let's import the necessary libraries. We'll need `torch` for building and training our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10ecceb90>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch # PyTorch: a library for deep learning\n",
    "from torch.nn import functional as F # functional: a module for built functions\n",
    "from transformers import GPT2LMHeadModel, GPT2Config # transformers: a library for untrained and pre-trained models\n",
    "import random # random: a module for generating random numbers\n",
    "import numpy as np # numpy: a library for numerical computing\n",
    "\n",
    "# For reproducibility: set the random seed to 1\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "We'll define some hyperparameters for our model and training process. Since we're running on CPU and want this to be quick for a tutorial, these values will be small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 32       # How many independent sequences will we process in parallel \n",
    "block_size = 10      # What is the maximum context length for predictions?\n",
    "max_iters = 5000      # Total training iterations\n",
    "eval_interval = 500   # How often to evaluate on validation set\n",
    "learning_rate = 5e-4  # Learning rate for the optimizer\n",
    "device = 'cpu'        # Explicitly set to CPU\n",
    "eval_iters = 100      # Number of iterations for evaluation\n",
    "n_embd = 128          # Embedding dimension (reduced for CPU)\n",
    "n_head = 4            # Number of attention heads (reduced for CPU)\n",
    "n_layer = 4           # Number of transformer blocks (reduced for CPU)\n",
    "dropout = 0.0          # Dropout rate>0 to prevent overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation\n",
    "\n",
    "We'll use a small text file as our dataset. For this tutorial, let's use a snippet of Shakespeare's writings.\n",
    "\n",
    "### 3.1 Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use a small snippet of text for this tutorial.\n",
    "# You can replace this with the path to a larger .txt file if you wish.\n",
    "text = \"\"\"To stale 't a little more.\n",
    "\n",
    "First Citizen:\n",
    "Well, I'll hear it, sir: yet you must not think to\n",
    "fob off our disgrace with a tale: but, an 't please\n",
    "you, deliver.\n",
    "\n",
    "MENENIUS:\n",
    "There was a time when all the body's members\n",
    "Rebell'd against the belly, thus accused it:\n",
    "That only like a gulf it did remain\n",
    "I' the midst o' the body, idle and unactive,\n",
    "Still cupboarding the viand, never bearing\n",
    "Like labour with the rest, where the other instruments\n",
    "Did see and hear, devise, instruct, walk, feel,\n",
    "And, mutually participate, did minister\n",
    "Unto the appetite and affection common\n",
    "Of the whole body. The belly answer'd--\n",
    "\n",
    "First Citizen:\n",
    "Well, sir, what answer made the belly?\n",
    "\n",
    "MENENIUS:\n",
    "Sir, I shall tell you. With a kind of smile,\n",
    "Which ne'er came from the lungs, but even thus--\n",
    "For, look you, I may make the belly smile\n",
    "As well as speak--it tauntingly replied\n",
    "To the discontented members, the mutinous parts\n",
    "That envied his receipt; even so most fitly\n",
    "As you malign our senators for that\n",
    "They are not such as you.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Word-level Tokenization\n",
    "\n",
    "Since this is a word-level model, our vocabulary will consist of all unique words present in the text. We'll create mappings from words to integers (encode) and integers to words (decode)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample words: [\"'\", ',', '-', '.', ':', ';', '?', 'a', 'accused', 'affection', 'against', 'all', 'an', 'and', 'answer', 'appetite', 'are', 'as', 'bearing', 'belly']\n",
      "Vocabulary size: 123\n"
     ]
    }
   ],
   "source": [
    "# Get all unique words in the text by regular expression\n",
    "import re \n",
    "\n",
    "# Simple word tokenization - split on whitespace and punctuation\n",
    "words = re.findall(r'\\b\\w+\\b|[^\\w\\s]', text.lower())\n",
    "unique_words = sorted(list(set(words)))\n",
    "vocab_size = len(unique_words)\n",
    "\n",
    "print(\"Sample words:\", unique_words[:20])\n",
    "print(f\"Vocabulary size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: what answer made you\n",
      "Encoded: [115, 14, 56, 122]\n",
      "Decoded:  what answer made you\n"
     ]
    }
   ],
   "source": [
    "# Create a mapping from words to integers and vice-versa\n",
    "stoi = {word: i for i, word in enumerate(unique_words)}\n",
    "itos = {i: word for i, word in enumerate(unique_words)}\n",
    "\n",
    "def encode(s):\n",
    "    \"\"\"Encoder: take a string, output a list of integers\"\"\"\n",
    "    words = re.findall(r'\\b\\w+\\b|[^\\w\\s]', s.lower())\n",
    "    return [stoi[word] for word in words if word in stoi]\n",
    "\n",
    "def decode(l):\n",
    "    \"\"\"Decoder: take a list of integers, output a string\"\"\"\n",
    "    words = [itos[i] for i in l]\n",
    "    # Simple reconstruction - join with spaces, handle punctuation\n",
    "    result = \"\"\n",
    "    for i, word in enumerate(words):\n",
    "        if word in \".,!?;:-'\":\n",
    "            result += word\n",
    "        else:\n",
    "            result += \" \" + word\n",
    "    return result\n",
    "\n",
    "# Test encoding and decoding\n",
    "test_text = \"what answer made you\"\n",
    "encoded = encode(test_text)\n",
    "decoded = decode(encoded)\n",
    "print(f\"Original: {test_text}\")\n",
    "print(f\"Encoded: {encoded}\")\n",
    "print(f\"Decoded: {decoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation: Tokenizer and Tokens\n",
    "\n",
    "A **tokenizer** is a crucial component in Natural Language Processing (NLP). Its primary job is to break down a piece of text (like a sentence or a paragraph) into smaller units called **tokens**. These tokens can be words, sub-words, or even individual characters, depending on the type of tokenizer used.\n",
    "\n",
    "Think of it like this: computers don't understand words and sentences directly. They understand numbers. So, a tokenizer first segments the text and then, in conjunction with a vocabulary mapping (like our `stoi` and `itos` dictionaries), converts these tokens into numerical representations that a machine learning model can process.\n",
    "\n",
    "In this notebook, we are using a **word-level tokenizer**. This means each token is a unique word or punctuation mark found in our text.\n",
    "- The `re.findall(r'\\b\\w+\\b|[^\\w\\s]', text.lower())` line is our simple tokenizer. It uses regular expressions to find sequences of word characters (`\\w+`) or any character that is not a word character or whitespace (`[^\\w\\s]`). This helps us capture both words and punctuation marks as separate tokens.\n",
    "- `stoi` (string to integer) maps each unique token (word/punctuation) to a unique integer.\n",
    "- `itos` (integer to string) does the reverse, mapping integers back to their original tokens.\n",
    "\n",
    "This process is fundamental because it allows the model to learn patterns and relationships between these numerical representations of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Create Training and Validation Splits\n",
    "\n",
    "We'll split our dataset into a training set and a validation set. The model learns from the training set, and we use the validation set to check how well it's generalizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: torch.Size([238]), Data type: torch.int64\n",
      "Sample encoded text: [108, 94, 0, 97, 7, 52, 64, 3, 36, 23, 4, 114, 1, 44, 0, 53, 42, 48, 1, 90]\n",
      "Sample decoded text:  to stale' t a little more. first citizen: well, i' ll hear it, sir\n",
      "Training data length: 119 words\n",
      "Validation data length: 119 words\n"
     ]
    }
   ],
   "source": [
    "# Encode the entire text dataset and store it into a torch.Tensor\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(f\"Data shape: {data.shape}, Data type: {data.dtype}\")\n",
    "print(f\"Sample encoded text: {data[:20].tolist()}\")\n",
    "print(f\"Sample decoded text: {decode(data[:20].tolist())}\")\n",
    "\n",
    "# Split up the data into train and validation sets\n",
    "n = int(0.5*len(data)) # first 50% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "print(f\"Training data length: {len(train_data)} words\")\n",
    "print(f\"Validation data length: {len(val_data)} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Data Loader\n",
    "\n",
    "We need a way to feed data to our model in batches. The `get_batch` function will randomly sample `batch_size` chunks of `block_size` length from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split): #  Data loading function, we would generate (X,Y)'s from the text just like normal ML training data\n",
    "    # Generate a small batch of data of inputs x and targets y\n",
    "    # X: the context of the words as the input\n",
    "    # Y: the next word as the target, Y[i] is the next word of X[i]\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    # batch size: the number of training examples in a batch for speeding up the training process\n",
    "    # block size: the maximum number of words in the context (X)\n",
    "    # If the block size is too large, there would be a large computational cost; if the block size is too small, the model would not be able to capture the context of the words\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,)) # randomly select the starting point of the context with the length of block_size\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix]) # X: the context of the words as the input\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix]) # Y: the next word as the target, Y[i] is the next word of X[i]\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of a batch:\n",
      "Context (X):  instruments did see and hear, devise, instruct,\n",
      "Target (Y):  did see and hear, devise, instruct, walk\n",
      "What happens when we train the model: each (X,Y) would be broken down into a sequence of (X'_t,Y'_t) pairs as the the real training data\n",
      "X': the context of the words as the input X'_t=X[:t+1] for each t\n",
      "Y': the next word as the target, Y'_t=Y[t] is the next word of X[:t+1] for each t\n",
      "Context (X'_0): ' instruments' -> Target (Y'_0): 'did'\n",
      "Context (X'_1): ' instruments did' -> Target (Y'_1): 'see'\n",
      "Context (X'_2): ' instruments did see' -> Target (Y'_2): 'and'\n",
      "Context (X'_3): ' instruments did see and' -> Target (Y'_3): 'hear'\n",
      "Context (X'_4): ' instruments did see and hear' -> Target (Y'_4): ','\n",
      "Context (X'_5): ' instruments did see and hear,' -> Target (Y'_5): 'devise'\n",
      "Context (X'_6): ' instruments did see and hear, devise' -> Target (Y'_6): ','\n",
      "Context (X'_7): ' instruments did see and hear, devise,' -> Target (Y'_7): 'instruct'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xb, yb = get_batch('train')\n",
    "# Example of a batch\n",
    "print(\"Example of a batch:\")\n",
    "for b in range(min(1, batch_size)): \n",
    "    print(f\"Context (X): {decode(xb[b].tolist())}\")\n",
    "    print(f\"Target (Y): {decode(yb[b].tolist())}\")\n",
    "\n",
    "\n",
    "print(\"What happens when we train the model: each (X,Y) would be broken down into a sequence of (X'_t,Y'_t) pairs as the the real training data\")\n",
    "print(\"X': the context of the words as the input X'_t=X[:t+1] for each t\")\n",
    "print(\"Y': the next word as the target, Y'_t=Y[t] is the next word of X[:t+1] for each t\")\n",
    "for b in range(min(1, batch_size)):  # Show1 examples\n",
    "    for t in range(min(8, block_size)):  # Show first 8 positions\n",
    "        context_words = decode(xb[b, :t+1].tolist())\n",
    "        target_word = itos[yb[b,t].item()]\n",
    "        print(f\"Context (X'_{t}): '{context_words}' -> Target (Y'_{t}): '{target_word}'\")\n",
    "        if t == 7:  # Only show first 8 examples\n",
    "            break\n",
    "    print()\n",
    "    if b == 1:  # Only show for first 2 batch items\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation: `get_batch` - Why X and Y have the same length\n",
    "\n",
    "In the `get_batch` function, both the input `x` and the target `y` are sequences of `block_size` length. This might seem counterintuitive at first, as we are trying to predict the *next* word. Let's clarify:\n",
    "\n",
    "The task of a language model is to predict the next token in a sequence, given the preceding tokens.\n",
    "- `x` represents the **input context**: a sequence of words the model sees.\n",
    "- `y` represents the **target output**: for each position in `x`, `y` contains the word that *immediately follows* it in the original text.\n",
    "\n",
    "Consider a `block_size` of 10.\n",
    "If `x` is `[word_1, word_2, ..., word_10]`,\n",
    "then `y` will be `[word_2, word_3, ..., word_11]`.\n",
    "\n",
    "So, for each element `x[i]` in the input sequence (at a specific time step `t` within the block), the corresponding `y[i]` is the actual next word that the model should have predicted.\n",
    "\n",
    "For example, when the model sees:\n",
    "- `x[0]` (i.e., `word_1`), it tries to predict `y[0]` (i.e., `word_2`).\n",
    "- `x[0:1]` (i.e., `word_1, word_2`), it tries to predict `y[1]` (i.e., `word_3`).\n",
    "- ...\n",
    "- `x[0:9]` (i.e., `word_1, ..., word_10`), it tries to predict `y[9]` (i.e., `word_11`).\n",
    "\n",
    "The model makes predictions for *every position* in the `block_size`. The loss function then compares all these predictions with the actual next words in `y`. This way, a single block of data provides `block_size` individual training examples for the model to learn from. The \"context\" for predicting `y[t]` is `x[0...t]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Definition (Simplified nanoGPT Style)\n",
    "\n",
    "Now we'll build a simplified version of the GPT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.81 M parameters\n"
     ]
    }
   ],
   "source": [
    "# Define the GPT-2 model configuration using transformers library\n",
    "config = GPT2Config(\n",
    "    vocab_size=vocab_size, # the number of unique words in the vocabulary, i.e., unique classes for the output layer\n",
    "    n_positions=block_size,  # the maximum number of words in the context (X)\n",
    "    n_embd=n_embd, # the number of embedding dimensions\n",
    "    n_layer=n_layer, # the number of layers\n",
    "    n_head=n_head, # the number of attention heads\n",
    "    resid_pdrop=dropout, # the dropout rate for the residual connections\n",
    "    embd_pdrop=dropout, # the dropout rate for the embedding layer\n",
    "    attn_pdrop=dropout, # the dropout rate for the attention layer\n",
    "    bos_token_id=None,  # No beginning of sentence token for word model\n",
    "    eos_token_id=None   # No end of sentence token for word model\n",
    ")\n",
    "\n",
    "# Instantiate the model: we use GPT2LMHeadModel, which is an (untrained) GPT-2 model for language modeling/next-token prediction\n",
    "model = GPT2LMHeadModel(config)\n",
    "model.to(device)\n",
    "\n",
    "# Print the number of parameters in the model\n",
    "print(f\"{sum(p.numel() for p in model.parameters())/1e6:.2f} M parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Optional) Explanation: Why No BOS/EOS Tokens? \n",
    "\n",
    "You might notice `bos_token_id` (Beginning of Sequence) and `eos_token_id` (End of Sequence) are set to `None`. This is a deliberate simplification for this tutorial.\n",
    "\n",
    "**Why They Are Crucial for Real LLMs:**\n",
    "- **To Define Boundaries:** In massive datasets with millions of documents, `BOS` and `EOS` tell the model where one document ends and another begins, preventing it from nonsensically connecting unrelated texts.\n",
    "- **To Control Generation:** A real chatbot or assistant needs to know when to stop talking. It learns to generate an `EOS` token when its response is complete.\n",
    "- **For Task Formatting:** These tokens separate the user's prompt from the model's desired output, helping the model learn the structure of a conversation or task.\n",
    "\n",
    "**Why We Don't Need Them Here:**\n",
    "- **Continuous Data:** We train on a single, continuous stream of text. The model isn't learning from separate documents where boundaries are important.\n",
    "- **Controlled Generation:** We manually start generation with a prompt and stop it after a fixed number of words. We don't rely on the model to decide when to stop.\n",
    "\n",
    "#### (Optional) How `dropout` helps as a regularization technique:\n",
    "Dropout is a simple yet effective regularization technique to combat overfitting in neural networks. Here's how it works during training:\n",
    "1.  At each training step, for every neuron in a layer where dropout is applied, it is \"dropped out\" (i.e., temporarily removed from the network) with a certain probability (`dropout` rate).\n",
    "2.  This means the neuron, along with all its incoming and outgoing connections, is ignored during the forward pass and backward pass for that particular training iteration.\n",
    "3.  The choice of which neurons to drop is random for each training iteration and each input example.\n",
    "\n",
    "**Why does this help?**\n",
    "-   **Prevents Co-adaptation:** Neurons become less reliant on specific other neurons because they can't be sure which ones will be active. This encourages each neuron to learn more robust features that are useful in conjunction with different random subsets of other neurons.\n",
    "-   **Ensemble Effect (Implicit):** Training with dropout can be seen as training a large number of thinned networks (networks with different subsets of neurons). At test time (when dropout is turned off), using the full network can be viewed as an approximation of averaging the predictions of these many thinned networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training the Model\n",
    "\n",
    "### 5.1 Optimizer\n",
    "We'll use the AdamW optimizer, a common choice for training transformer models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Loss Estimation\n",
    "We need a function to estimate the loss on the training and validation sets without calculating gradients, which is useful for monitoring training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad() # Decorator to disable gradient calculation \n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters) #eval_iters: the number of iterations for evaluation\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            outputs = model(X, labels=Y)\n",
    "            loss = outputs.loss # loss: the loss of the model, i.e., the difference between the predicted and the actual labels values. This is automatically calculated by the model.\n",
    "            losses[k] = loss.item() # loss.item(): convert the loss to a scalar value\n",
    "        out[split] = losses.mean()\n",
    "    model.train() # Set model back to training mode\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Training Loop\n",
    "This is the main loop where the model learns. For each iteration, we:\n",
    "1. Get a batch of data.\n",
    "2. Perform a forward pass (get model predictions).\n",
    "3. Calculate the loss (how wrong the predictions are).\n",
    "4. Perform a backward pass (calculate gradients).\n",
    "5. Update the model's parameters using the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cpu...\n",
      "step 0: train loss 4.8432, val loss 4.8423\n",
      "step 500: train loss 0.1071, val loss 6.8329\n",
      "step 1000: train loss 0.0881, val loss 7.4115\n",
      "step 1500: train loss 0.0837, val loss 7.8524\n",
      "step 2000: train loss 0.0821, val loss 8.1622\n",
      "step 2500: train loss 0.0795, val loss 8.4942\n",
      "step 3000: train loss 0.0800, val loss 8.7935\n",
      "step 3500: train loss 0.0789, val loss 8.9024\n",
      "step 4000: train loss 0.0810, val loss 8.9835\n",
      "step 4500: train loss 0.0770, val loss 9.2195\n",
      "step 4999: train loss 0.0838, val loss 9.3415\n",
      "Training finished!\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training on {device}...\")\n",
    "\n",
    "for iter_num in range(max_iters):\n",
    "\n",
    "    # Evaluation part\n",
    "    if iter_num % eval_interval == 0 or iter_num == max_iters - 1: \n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # Training part\n",
    "    xb, yb = get_batch('train') # 1. Sample a batch of data for training\n",
    "\n",
    "    \n",
    "    outputs = model(xb, labels=yb) # 2. Foward pass: the model predicts the next word of the context\n",
    "    loss = outputs.loss # 3. Calculate the loss of the model\n",
    "    \n",
    "    #4 and 5. Backward pass and optimization step\n",
    "    optimizer.zero_grad(set_to_none=True) # Zero out gradients from previous step\n",
    "    loss.backward() # Compute gradients for the current batch\n",
    "    optimizer.step() # Update model parameters\n",
    "\n",
    "print(\"Training finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Optional) Explanation: The Training Loop - How a Model Learns\n",
    "\n",
    "The training loop is the core process where the model \"learns\" by repeatedly guessing, checking how wrong its guess is, and then slightly improving its guess. To understand this, let's use a simple analogy: **finding the average of a list of numbers** using Gradient Descent.\n",
    "\n",
    "Imagine our \"dataset\" is the list of numbers `[1, 2, 3, 4, 5]`. The true average is 3.\n",
    "Our \"model\" is not a complex LLM, but just a single parameter: our current **guess** for the average, which we'll call `m`.\n",
    "\n",
    "The training process follows four key steps, repeated over and over.\n",
    "\n",
    "**Step 1: Initialize**\n",
    "First, we make a random guess. Let's say we initialize our model with `m = 0`. This is the equivalent of the LLM's millions of parameters starting as random numbers.\n",
    "\n",
    "**Step 2: The Forward Pass (Predict & Calculate Loss)**\n",
    "We see how wrong our guess is. We take a data point (e.g., the number `4`), compare it to our prediction (`m=0`), and calculate the error, or **loss**. A common loss function is the squared error.\n",
    "\n",
    "-   **Prediction:** Our current guess, `0`.\n",
    "-   **Actual Value:** The data point, `4`.\n",
    "-   **Loss:** `(prediction - actual)^2 = (0 - 4)^2 = 16`.\n",
    "-   *LLM Equivalent:* This is `outputs = model(xb, labels=yb)`. The model takes an input `xb`, makes a prediction, and the loss is calculated by comparing it to the correct answer `yb`. This entire process is the **Forward Pass**.\n",
    "\n",
    "**Step 3: The Backward Pass (Calculate Gradients)**\n",
    "Now for the magic: how do we improve our guess? The **gradient** tells us the direction of the error. For our loss `(m - 4)^2`, the gradient is `2 * (m - 4)`.\n",
    "\n",
    "-   **Gradient:** `2 * (0 - 4) = -8`.\n",
    "-   **What this means:** The negative sign tells us we need to *increase* `m` to reduce the error. The magnitude (`8`) tells us we are far from the correct answer.\n",
    "-   *LLM Equivalent:* `loss.backward()` does this for every single one of the model's millions of parameters. It calculates whether each parameter should be nudged up or down to reduce the final loss. This is the **Backward Pass**.\n",
    "\n",
    "**Step 4: The Update Step (Adjust the Model)**\n",
    "We use the gradient to update our guess. We take a small step in the right direction, controlled by a `learning_rate` (e.g., 0.1).\n",
    "\n",
    "-   **Update Rule:** `new_m = old_m - learning_rate * gradient`\n",
    "-   **New Guess:** `m = 0 - 0.1 * (-8) = 0.8`.\n",
    "-   Our new guess, 0.8, is much closer to the true average of 3. We have learned!\n",
    "-   *LLM Equivalent:* `optimizer.step()` performs this update for all parameters. (AdamW is just a more advanced version of this same principle).\n",
    "\n",
    "The **training loop** simply repeats these three steps (Forward Pass, Backward Pass, Update) thousands of times. With each iteration, the model's parameters get slightly less wrong, until they converge to values that produce accurate and coherent text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation: The Overfitting Issue\n",
    "\n",
    "**Overfitting** is a common problem in machine learning. It occurs when a model learns the training data *too well*, including its noise and specific details, to the point where it performs poorly on new, unseen data (like our validation set or real-world data).\n",
    "\n",
    "Imagine a student who memorizes the answers to all the questions in a specific textbook (the training data) but doesn't understand the underlying concepts. When given a new exam with slightly different questions (the validation or test data), the student would likely fail. This is analogous to an overfitted model.\n",
    "\n",
    "**Signs of Overfitting:**\n",
    "-   **Training loss continues to decrease, while validation loss starts to increase or plateaus.** This is a classic indicator. It means the model is getting better at fitting the training data but worse (or no better) at generalizing to unseen data.\n",
    "-   The model might produce excellent results on examples it has seen during training but generate nonsensical or poor outputs for new inputs.\n",
    "\n",
    "\n",
    "**In our case, there is another reason (and might be the major reason)** for the poor validation loss: since the training data (the first part of the text) is very different from the testing data (the second part), the LLM cannot learn to predict the testing data from training. This is also called the out-of-distribution (OOD) issue.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generating Text\n",
    "\n",
    "Now that our model is trained, let's use it to generate some text. We'll start with a context (e.g., a newline word) and ask the model to predict the next words sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input context: ' sir: yet you must not think to fob off our disgrace with a tale'\n"
     ]
    }
   ],
   "source": [
    "# Generation function - create a simple generation loop\n",
    "\n",
    "\n",
    "# Start with the first few words from our training data\n",
    "text_test=\"sir: yet you must not think to fob off our disgrace with a tale\"\n",
    "context = torch.tensor(encode(text_test), \n",
    "                      dtype=torch.long, device=device).unsqueeze(0) # unsqueeze(0): add a dimension at the beginning of the tensor to match the format of the training data\n",
    "#Training data shape for X: (batch_size, block_size)\n",
    "#Context shape in X: (1, block_size)\n",
    "\n",
    "print(f\"Input context: '{decode(context[0].tolist())}'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input context: ' sir: yet you must not think to fob off our disgrace with a tale'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generated Text ---\n",
      " sir: yet you must not think to fob off our disgrace with a tale but an t you deliver menenius there a more first: was time all body s rebell d the,' against belly thus it remain' midst' accused:, accused:,' hear,,,,,,,,: was' hear,\n",
      "--- Original Text ---\n",
      "sir: yet you must not think to fob off our disgrace with a tale: but, an 't please you, deliver. MENENIUS: There was a time when all the body's members Rebell'd against the belly, thus accused it: That only like a gulf it did remain\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input context: '{decode(context[0].tolist())}'\")\n",
    "# Generation loop to generate text token by token\n",
    "\n",
    "generated = context #initialize the generated sequence with the context\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for _ in range(50):  # Generate 50 more words\n",
    "        # Get predictions for the current context\n",
    "        # Crop to last block_size tokens if context gets too long\n",
    "        context_cropped = generated[:, -block_size:] if generated.size(1) > block_size else generated\n",
    "        \n",
    "        outputs = model(context_cropped) # forward pass: the model predicts the next word of the context\n",
    "        logits = outputs.logits[:, -1, :] # logits: the output of the model, the scores of each word in the vocabulary to be the next word of the context\n",
    "        \n",
    "        # Apply temperature for sampling (Optional)\n",
    "        # temperature = 1\n",
    "        # logits = logits / temperature\n",
    "        \n",
    "        # Sample from the distribution\n",
    "        probs = F.softmax(logits, dim=-1) # softmax: convert the logits to probabilities over the vocabulary, exp(logits_i) / sum_j(exp(logits_j))\n",
    "        next_token = torch.multinomial(probs, num_samples=1) # multinomial: sample from the distribution, i.e., the next word of the context\n",
    "        \n",
    "        # Append to generated sequence\n",
    "        generated = torch.cat([generated, next_token], dim=1) # concatenate the generated sequence and the next word of the context\n",
    "\n",
    "        \n",
    "# Decode the generated text\n",
    "generated_text = decode(generated[0].tolist())\n",
    "print(\"--- Generated Text ---\")\n",
    "print(generated_text)\n",
    "print(\"--- Original Text ---\")\n",
    "print('''sir: yet you must not think to fob off our disgrace with a tale: but, an 't please you, deliver. MENENIUS: There was a time when all the body's members Rebell'd against the belly, thus accused it: That only like a gulf it did remain''')\n",
    "print(\"----------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation: Sampling in Text Generation\n",
    "\n",
    "When our model generates text, it doesn't just pick the single most probable next word. Instead, it often uses a technique called **sampling**, sometimes influenced by a parameter called **temperature**.\n",
    "\n",
    "1.  **Logits and Probabilities:**\n",
    "    After processing the input context, the model outputs **logits**. These are raw, unnormalized scores for each word in the vocabulary. To turn these logits into probabilities (i.e., values between 0 and 1 that sum up to 1), we apply a **softmax function**. The word with the highest probability is the one the model thinks is most likely to come next.\n",
    "\n",
    "2.  **Sampling:**\n",
    "    Instead of always picking the word with the absolute highest probability (which is called greedy decoding and can lead to repetitive or boring text), we can *sample* from the probability distribution. This means a word that is slightly less probable might still be chosen, introducing randomness and creativity into the generated text.\n",
    "    `torch.multinomial(probs, num_samples=1)` is the function that performs this sampling. It takes the probability distribution (`probs`) and picks one word based on these probabilities.\n",
    "\n",
    "3.  **(Optional) Temperature:**\n",
    "    Temperature is a hyperparameter that controls the randomness of the sampling. It's applied to the logits *before* the softmax function.\n",
    "    -   **Low Temperature (e.g., < 1.0, closer to 0):** Dividing logits by a small number makes the differences between them larger. When softmax is applied, the probability distribution becomes \"sharper\" or \"spikier.\" The model becomes more confident and deterministic, tending to pick the most likely words. This can lead to more focused and coherent text, but potentially less creative.\n",
    "    -   **High Temperature (e.g., > 1.0):** Dividing logits by a larger number makes the differences between them smaller. The resulting softmax probability distribution becomes \"flatter\" or \"softer.\" The model becomes less confident, and less likely words have a higher chance of being selected. This increases randomness and creativity, but can also lead to more errors or nonsensical text.\n",
    "    -   **Temperature = 1.0:** This is the standard setting where the original probabilities are used for sampling.\n",
    "\n",
    "In our generation code:\n",
    "`logits = logits / temperature`\n",
    "`probs = F.softmax(logits, dim=-1)`\n",
    "\n",
    "By adjusting the `temperature` value, you can control the trade-off between coherence and creativity in the generated text. For this tutorial, we use a temperature of 1.0, which means we sample directly from the model's learned probability distribution."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GenAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
